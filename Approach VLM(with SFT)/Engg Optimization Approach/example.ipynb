{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9006034",
   "metadata": {},
   "source": [
    "##  Basic Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U transformers datasets peft trl bitsandbytes accelerate qwen-vl-utils pillow tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432e4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "import torch\n",
    "from PIL import Image\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"UNSLOTH_DISABLE_TRAINER_PATCHING\"] = \"1\"  \n",
    "os.environ[\"UNSLOTH_NO_CUDA_EXTENSIONS\"] = \"1\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATASET_FOLDER = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ae1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Full Training Mode: True\n",
      "  Run Inference: False\n",
      "  Skip Dataset Creation: False\n",
      "  Skip Image Download: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndevelopment (validation mode):\\nFULL_TRAINING_MODE = False\\nINFERENCE_ONLY = False\\nRUN_INFERENCE = False\\nSKIP_DATASET_CREATION = False\\nSKIP_IMAGE_DOWNLOAD = False\\n\\nfull training:\\nFULL_TRAINING_MODE = True   # Use all data\\nINFERENCE_ONLY = False\\nRUN_INFERENCE = False\\nSKIP_DATASET_CREATION = True   # Reuse\\nSKIP_IMAGE_DOWNLOAD = True     # Reuse\\n\\ngenerate submission:\\nFULL_TRAINING_MODE = True   # Doesn't matter\\nINFERENCE_ONLY = True       # Skip training!\\nRUN_INFERENCE = True        # Generate predictions\\nSKIP_DATASET_CREATION = True\\nSKIP_IMAGE_DOWNLOAD = True\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FULL_TRAINING_MODE = True\n",
    "\n",
    "# Set to True to run inference on test set and generate submission\n",
    "RUN_INFERENCE = False\n",
    "\n",
    "# Set to True to skip dataset creation if already created\n",
    "SKIP_DATASET_CREATION = False\n",
    "\n",
    "# Set to True to skip image downloading if already done\n",
    "SKIP_IMAGE_DOWNLOAD = False\n",
    "\n",
    "INFERENCE_CHECKPOINT = None  # None = use default output_dir\n",
    "\n",
    "INFERENCE_ONLY = False\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "\n",
    "\n",
    "# Options for resuming:\n",
    "# RESUME_FROM_CHECKPOINT = \"./qwen2-vl-7b-price-predictor-best/periodic_checkpoint\"  # Latest periodic\n",
    "# RESUME_FROM_CHECKPOINT = \"./qwen2-vl-7b-price-predictor-best/checkpoint-epoch-1\"  # Specific epoch\n",
    "# RESUME_FROM_CHECKPOINT = True  # Auto-detect latest checkpoint\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Full Training Mode: {FULL_TRAINING_MODE}\")\n",
    "print(f\"  Run Inference: {RUN_INFERENCE}\")\n",
    "print(f\"  Skip Dataset Creation: {SKIP_DATASET_CREATION}\")\n",
    "print(f\"  Skip Image Download: {SKIP_IMAGE_DOWNLOAD}\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "development (validation mode):\n",
    "FULL_TRAINING_MODE = False\n",
    "INFERENCE_ONLY = False\n",
    "RUN_INFERENCE = False\n",
    "SKIP_DATASET_CREATION = False\n",
    "SKIP_IMAGE_DOWNLOAD = False\n",
    "\n",
    "full training:\n",
    "FULL_TRAINING_MODE = True   # Use all data\n",
    "INFERENCE_ONLY = False\n",
    "RUN_INFERENCE = False\n",
    "SKIP_DATASET_CREATION = True   # Reuse\n",
    "SKIP_IMAGE_DOWNLOAD = True     # Reuse\n",
    "\n",
    "generate submission:\n",
    "FULL_TRAINING_MODE = True   # Doesn't matter\n",
    "INFERENCE_ONLY = True       # Skip training!\n",
    "RUN_INFERENCE = True        # Generate predictions\n",
    "SKIP_DATASET_CREATION = True\n",
    "SKIP_IMAGE_DOWNLOAD = True\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81b430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loading Pre-Split Datasets...\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading training data from train_split.jsonl...\n",
      "‚úÖ Loaded 67499 training samples\n",
      "\n",
      "üìÇ Loading validation data from validation_split.jsonl...\n",
      "‚úÖ Loaded 7500 validation samples\n",
      "\n",
      "============================================================\n",
      "‚úÖ Datasets ready for DDP training!\n",
      "Training samples: 67499\n",
      "Validation samples: 7500\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Pre-Split Datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training split\n",
    "print(\"\\nüìÇ Loading training data from train_split.jsonl...\")\n",
    "train_dataset = []\n",
    "with open(f\"{DATASET_FOLDER}/train_split.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        try:\n",
    "            train_dataset.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Skipping malformed JSON on line {line_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_dataset)} training samples\")\n",
    "\n",
    "# Load validation split\n",
    "print(\"\\nüìÇ Loading validation data from validation_split.jsonl...\")\n",
    "validation_dataset = []\n",
    "with open(f\"{DATASET_FOLDER}/validation_split.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        try:\n",
    "            validation_dataset.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Skipping malformed JSON on line {line_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(validation_dataset)} validation samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Datasets ready for DDP training!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(validation_dataset)}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591f7fa",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from unsloth import FastVisionModel\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "train_dataset_path = \"./train_split.jsonl\"\n",
    "validation_dataset_path = \"./validation_split.jsonl\"\n",
    "output_dir = \"./qwen2.5-vl-3b-price-predictor-best\"\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917f49c-043b-46c1-acd7-c17964535598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import TrainerCallback\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "class SMAPEEvaluationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    DDP-aware SMAPE evaluation callback.\n",
    "    - Runs SMAPE computation ONLY on rank 0 (main GPU)\n",
    "    - All other GPUs wait at a barrier\n",
    "    - Evaluates every `eval_steps` steps\n",
    "    - NO early stopping (just monitoring)\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_dataset, processor, eval_steps=150):\n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.processor = processor\n",
    "        self.eval_steps = eval_steps\n",
    "        self.smape_history = []\n",
    "        self.best_smape = float('inf')\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Only evaluate at specified intervals\n",
    "        if state.global_step % self.eval_steps != 0:\n",
    "            return control\n",
    "        \n",
    "        # Check if we're in DDP mode\n",
    "        is_distributed = dist.is_initialized()\n",
    "        is_main_process = not is_distributed or dist.get_rank() == 0\n",
    "        \n",
    "        if is_main_process:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"üéØ SMAPE Evaluation at Step {state.global_step}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "            \n",
    "            model = kwargs.get(\"model\")\n",
    "            if model is None:\n",
    "                print(\"‚ö†Ô∏è Warning: Model not found in callback kwargs\")\n",
    "                if is_distributed:\n",
    "                    dist.barrier()  # Still need to sync with other processes\n",
    "                return control\n",
    "            \n",
    "            # Set model to eval mode\n",
    "            model.eval()\n",
    "            \n",
    "            predictions = []\n",
    "            actuals = []\n",
    "            \n",
    "            # Run inference on validation set\n",
    "            with torch.no_grad():\n",
    "                for i, example in enumerate(self.validation_dataset):\n",
    "                    try:\n",
    "                        # Prepare input\n",
    "                        messages = example[\"messages\"]\n",
    "                        text = self.processor.apply_chat_template(\n",
    "                            messages, tokenize=False, add_generation_prompt=True\n",
    "                        )\n",
    "                        \n",
    "                        # Extract images\n",
    "                        images = []\n",
    "                        for msg in messages:\n",
    "                            if isinstance(msg[\"content\"], list):\n",
    "                                for item in msg[\"content\"]:\n",
    "                                    if isinstance(item, dict) and item.get(\"type\") == \"image\":\n",
    "                                        img_path = item[\"image\"]\n",
    "                                        try:\n",
    "                                            img = Image.open(img_path).convert(\"RGB\")\n",
    "                                            images.append(img)\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"‚ö†Ô∏è Error loading image {img_path}: {e}\")\n",
    "                                            continue\n",
    "                        \n",
    "                        # Process inputs\n",
    "                        inputs = self.processor(\n",
    "                            text=[text],\n",
    "                            images=[images if images else None],\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,\n",
    "                        ).to(model.device)\n",
    "                        \n",
    "                        # Generate prediction\n",
    "                        generated_ids = model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=20,\n",
    "                            num_beams=1,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                            eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                        )\n",
    "                        \n",
    "                        # Decode prediction\n",
    "                        generated_ids_trimmed = generated_ids[0][len(inputs.input_ids[0]):]\n",
    "                        prediction = self.processor.decode(\n",
    "                            generated_ids_trimmed,\n",
    "                            skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=False,\n",
    "                        ).strip()\n",
    "                        \n",
    "                        # Parse prediction\n",
    "                        try:\n",
    "                            pred_price = float(prediction)\n",
    "                        except (ValueError, TypeError):\n",
    "                            pred_price = 0.0\n",
    "                        \n",
    "                        # Get actual price from assistant response\n",
    "                        actual_price = None\n",
    "                        for msg in messages:\n",
    "                            if msg[\"role\"] == \"assistant\":\n",
    "                                try:\n",
    "                                    actual_price = float(msg[\"content\"])\n",
    "                                except (ValueError, TypeError):\n",
    "                                    continue\n",
    "                        \n",
    "                        if actual_price is not None:\n",
    "                            predictions.append(pred_price)\n",
    "                            actuals.append(actual_price)\n",
    "                        \n",
    "                        # Clean up\n",
    "                        del inputs, generated_ids\n",
    "                        \n",
    "                        # Progress indicator (every 50 samples)\n",
    "                        if (i + 1) % 50 == 0:\n",
    "                            print(f\"  Evaluated {i + 1}/{len(self.validation_dataset)} samples...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Error processing sample {i}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Calculate SMAPE\n",
    "            if len(predictions) > 0:\n",
    "                smape = self._calculate_smape(actuals, predictions)\n",
    "                self.smape_history.append(smape)\n",
    "                \n",
    "                # Update best SMAPE\n",
    "                if smape < self.best_smape:\n",
    "                    self.best_smape = smape\n",
    "                    print(f\"\\nüéâ New best SMAPE: {smape:.2f}% (improved!)\")\n",
    "                else:\n",
    "                    print(f\"\\nüìä Current SMAPE: {smape:.2f}% (best: {self.best_smape:.2f}%)\")\n",
    "                \n",
    "                print(f\"Samples evaluated: {len(predictions)}\")\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è No valid predictions - SMAPE not calculated\")\n",
    "            \n",
    "            # Cleanup memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Set model back to training mode\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"{'=' * 60}\\n\")\n",
    "        \n",
    "        # CRITICAL: Synchronize all processes\n",
    "        # Non-main processes wait here while rank 0 does SMAPE eval\n",
    "        if is_distributed:\n",
    "            dist.barrier()  # All GPUs must reach this point before continuing\n",
    "            if not is_main_process:\n",
    "                print(f\"[Rank {dist.get_rank()}] ‚è∏Ô∏è  Waited for SMAPE evaluation to complete\")\n",
    "        \n",
    "        return control\n",
    "    \n",
    "    def _calculate_smape(self, actual, predicted):\n",
    "        \"\"\"Calculate Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "        actual = np.array(actual)\n",
    "        predicted = np.array(predicted)\n",
    "        \n",
    "        numerator = np.abs(predicted - actual)\n",
    "        denominator = (np.abs(actual) + np.abs(predicted)) / 2\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        mask = denominator != 0\n",
    "        smape_values = np.zeros_like(numerator)\n",
    "        smape_values[mask] = numerator[mask] / denominator[mask]\n",
    "        \n",
    "        return np.mean(smape_values) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bfad0-a46e-46f5-930d-eefcdeec4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "print(\"üöÄ Loading Qwen2.5-VL-3B with Unsloth optimization...\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    max_seq_length=384,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    # use_gradient_checkpointing=True,\n",
    "    use_gradient_checkpointing=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Enable training mode\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    # use_gradient_checkpointing=\"unsloth\",\n",
    "    use_gradient_checkpointing=False,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading processor...\")\n",
    "min_pix = 256 * 28 * 28\n",
    "max_pix = 512 * 28 * 28\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, min_pixels=min_pix, max_pixels=max_pix, trust_remote_code=True)\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "print(f\"   EOS token: '{processor.tokenizer.eos_token}' (ID: {processor.tokenizer.eos_token_id})\")\n",
    "print(f\"   PAD token: '{processor.tokenizer.pad_token}' (ID: {processor.tokenizer.pad_token_id})\")\n",
    "print(\"Processor loaded successfully!\")\n",
    "print(f\"Vocab size: {len(processor.tokenizer)}\")\n",
    "print(f\"   Min pixels: {min_pix:,} (~256x256 after resize)\")\n",
    "print(f\"   Max pixels: {max_pix:,} (~512x512 after resize)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28579b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Custom data collator for Qwen2-VL multimodal inputs.\n",
    "    \n",
    "    What it does:\n",
    "    1. Applies chat template to convert messages to text format\n",
    "    2. Extracts image paths from message structure\n",
    "    3. Processes both text and images together with the processor\n",
    "    4. Creates labels for causal language modeling\n",
    "    \"\"\"\n",
    "    texts = [\n",
    "        processor.apply_chat_template(\n",
    "            example[\"messages\"], \n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for example in examples\n",
    "    ]\n",
    "\n",
    "    image_inputs = []\n",
    "    for example in examples:\n",
    "        images = []\n",
    "        for msg in example[\"messages\"]:\n",
    "            if isinstance(msg[\"content\"], list):\n",
    "                for item in msg[\"content\"]:\n",
    "                    if isinstance(item, dict) and item.get(\"type\") == \"image\":\n",
    "                        try:\n",
    "                            img = Image.open(item[\"image\"]).convert(\"RGB\")\n",
    "                            images.append(img)\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è Error loading image: {e}\")\n",
    "                            continue\n",
    "        image_inputs.append(images if images else None)\n",
    "\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "    \n",
    "    return batch\n",
    "\n",
    "print(\"Custom collator function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    # Basic settings\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    # Batch size (A100-optimized)\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch = 16\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=False,\n",
    "    # A100-specific optimizations\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    tf32=True,  # A100 TensorFloat-32\n",
    "    bf16=True,\n",
    "    # Training hyperparameters\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    # Logging\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    "    # Evaluation & checkpointing\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    # VLM-specific (CRITICAL!)\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    max_length=384,\n",
    "    packing=False,\n",
    "    # A100 performance\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_persistent_workers=True,\n",
    "    dataloader_prefetch_factor=8,\n",
    "    # Reproducibility\n",
    "    torch_empty_cache_steps=20,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(\n",
    "    f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\"\n",
    ")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision: bf16={training_args.bf16}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18c538-0319-4f4e-9286-7445b412d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    \"\"\"Clean up GPU memory between epochs\"\"\"\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"\\nüßπ GPU memory cleaned up\")\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8aa639-919c-451d-89d1-66e43b908e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import signal\n",
    "import torch.distributed as dist\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class PeriodicCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    DDP-safe periodic checkpoint callback.\n",
    "    - Saves every N steps (step-based)\n",
    "    - Only rank 0 saves the checkpoint\n",
    "    - Other ranks wait at barrier\n",
    "    - Handles keyboard interrupt gracefully\n",
    "    \"\"\"\n",
    "    def __init__(self, save_steps=250, checkpoint_dir=\"periodic_checkpoint\"):\n",
    "        self.save_steps = save_steps\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.last_save_step = 0\n",
    "        self.saving_in_progress = False\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Check if it's time to save\n",
    "        if state.global_step - self.last_save_step < self.save_steps:\n",
    "            return control\n",
    "        \n",
    "        # Check if we're in DDP mode\n",
    "        is_distributed = dist.is_initialized()\n",
    "        is_main_process = not is_distributed or dist.get_rank() == 0\n",
    "        \n",
    "        if is_main_process:\n",
    "            self.saving_in_progress = True  # Flag to prevent Ctrl+C during save\n",
    "            \n",
    "            checkpoint_path = os.path.join(args.output_dir, self.checkpoint_dir)\n",
    "            \n",
    "            # Remove old periodic checkpoint\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                print(f\"\\nüîÑ Replacing periodic checkpoint at step {state.global_step}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "            else:\n",
    "                print(f\"\\nüíæ Creating periodic checkpoint at step {state.global_step}\")\n",
    "            \n",
    "            # Get model from kwargs\n",
    "            model = kwargs.get(\"model\")\n",
    "            if model is None:\n",
    "                print(\"‚ö†Ô∏è Warning: Model not found in callback kwargs, skipping checkpoint\")\n",
    "                self.saving_in_progress = False\n",
    "                if is_distributed:\n",
    "                    dist.barrier()\n",
    "                return control\n",
    "            \n",
    "            try:\n",
    "                # Save model checkpoint\n",
    "                model.save_pretrained(checkpoint_path)\n",
    "                \n",
    "                # Save trainer state\n",
    "                state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "                with open(state_path, 'w') as f:\n",
    "                    state_dict = {\n",
    "                        'epoch': state.epoch,\n",
    "                        'global_step': state.global_step,\n",
    "                        'max_steps': state.max_steps,\n",
    "                        'num_train_epochs': state.num_train_epochs,\n",
    "                        'log_history': state.log_history[-10:],  # Keep last 10 entries\n",
    "                        'best_metric': state.best_metric,\n",
    "                        'best_model_checkpoint': state.best_model_checkpoint,\n",
    "                    }\n",
    "                    json.dump(state_dict, f, indent=2)\n",
    "                \n",
    "                print(f\"‚úÖ Periodic checkpoint saved (step {state.global_step})\")\n",
    "                self.last_save_step = state.global_step\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error saving checkpoint: {e}\")\n",
    "            finally:\n",
    "                self.saving_in_progress = False\n",
    "        \n",
    "        # CRITICAL: Synchronize all processes\n",
    "        if is_distributed:\n",
    "            dist.barrier()  # All GPUs wait until rank 0 finishes saving\n",
    "            if not is_main_process:\n",
    "                print(f\"[Rank {dist.get_rank()}] ‚è∏Ô∏è  Waited for checkpoint save to complete\")\n",
    "        \n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf567c4-591c-45a2-becf-74344472fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import sys\n",
    "\n",
    "class GracefulInterruptHandler:\n",
    "    \"\"\"\n",
    "    Handles Ctrl+C gracefully - waits if checkpoint is being saved\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_callback):\n",
    "        self.checkpoint_callback = checkpoint_callback\n",
    "        self.interrupted = False\n",
    "        signal.signal(signal.SIGINT, self.handle_interrupt)\n",
    "    \n",
    "    def handle_interrupt(self, signum, frame):\n",
    "        if self.checkpoint_callback.saving_in_progress:\n",
    "            print(\"\\n‚ö†Ô∏è  Checkpoint save in progress... Please wait!\")\n",
    "            print(\"(Forcing exit may corrupt the checkpoint)\")\n",
    "            return\n",
    "        \n",
    "        self.interrupted = True\n",
    "        print(\"\\nüõë Keyboard interrupt received. Stopping training gracefully...\")\n",
    "        print(\"(Last periodic checkpoint is safe to use)\")\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=collate_fn,\n",
    "    processing_class=processor.tokenizer,\n",
    ")\n",
    "trainer.add_callback(MemoryCleanupCallback())\n",
    "\n",
    "# ADD PERIODIC CHECKPOINT CALLBACK (Always enabled!)\n",
    "periodic_checkpoint = PeriodicCheckpointCallback(\n",
    "    save_steps=250,  # Save every 10 steps\n",
    "    checkpoint_dir=\"periodic_checkpoint\"  # Single rolling checkpoint\n",
    ")\n",
    "trainer.add_callback(periodic_checkpoint)\n",
    "interrupt_handler = GracefulInterruptHandler(periodic_checkpoint)\n",
    "smape_callback = SMAPEEvaluationCallback(\n",
    "    validation_dataset=validation_dataset,\n",
    "    processor=processor,\n",
    "    eval_steps=150  # Evaluate every 150 steps\n",
    ")\n",
    "trainer.add_callback(smape_callback)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Periodic checkpoint callback added (every 10 steps)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a05bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INFERENCE_ONLY:    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Starting Fine-Tuning...\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "    if RESUME_FROM_CHECKPOINT:\n",
    "        print(f\"üîÑ Resuming from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training Complete!\")\n",
    "    if smape_callback:\n",
    "        print(f\"Best SMAPE achieved: {smape_callback.best_smape:.2f}%\")\n",
    "        print(f\"SMAPE history: {smape_callback.smape_history}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total epochs: {trainer.state.epoch}\")\n",
    "    print(f\"Total steps: {trainer.state.global_step}\")\n",
    "    print(f\"Best checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "\n",
    "    if smape_callback:\n",
    "        print(f\"\\nSMAPE Performance:\")\n",
    "        print(f\"  Initial SMAPE: {smape_callback.smape_history[0]:.2f}%\")\n",
    "        print(f\"  Final SMAPE: {smape_callback.smape_history[-1]:.2f}%\")\n",
    "        print(f\"  Best SMAPE: {smape_callback.best_smape:.2f}%\")\n",
    "        print(\n",
    "            f\"  Improvement: {smape_callback.smape_history[0] - smape_callback.best_smape:.2f}%\"\n",
    "        )\n",
    "        print(f\"\\n  Epoch-by-epoch SMAPE:\")\n",
    "        for i, smape in enumerate(smape_callback.smape_history, 1):\n",
    "            print(f\"    Epoch {i}: {smape:.2f}%\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Saving the Best Model...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"\\n*** Model and processor saved to: {output_dir}\")\n",
    "    print(\"\\nSaved files:\")\n",
    "    print(\"  - adapter_config.json (LoRA configuration)\")\n",
    "    print(\"  - adapter_model.safetensors (LoRA weights)\")\n",
    "    print(\"  - tokenizer files\")\n",
    "    print(\"  - processor configuration\")\n",
    "else:\n",
    "    print(\"\\n***  Skipping training (INFERENCE_ONLY=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6508d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "import gc\n",
    "\n",
    "if RUN_INFERENCE:\n",
    "    is_distributed = dist.is_initialized()\n",
    "    is_main_process = not is_distributed or dist.get_rank() == 0\n",
    "\n",
    "    if is_main_process:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Running Inference on Test Set...\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "        test_dataset = load_dataset(\n",
    "            \"json\", data_files=f\"{DATASET_FOLDER}/test.jsonl\", split=\"train\"\n",
    "        )\n",
    "        print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "        checkpoint_path = INFERENCE_CHECKPOINT if INFERENCE_CHECKPOINT else output_dir\n",
    "        if os.path.exists(f\"{checkpoint_path}/adapter_model.safetensors\"):\n",
    "            print(f\"\\n*** Loading adapter from: {checkpoint_path}\")\n",
    "            from peft import PeftModel\n",
    "    \n",
    "            try:\n",
    "                _ = model.device\n",
    "            except:\n",
    "                print(\"Loading base model...\")\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                )\n",
    "                model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                    model_id,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "    \n",
    "            model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "            model.eval()\n",
    "            print(\"*** Fine-tuned adapter loaded!\")\n",
    "        else:\n",
    "            print(\"*** Using model from training session (no saved adapter found)\")\n",
    "            model.eval()\n",
    "    \n",
    "        predictions = []\n",
    "        sample_ids = []\n",
    "        batch_size = 16\n",
    "    \n",
    "        for batch_start in tqdm(range(0, len(test_dataset), batch_size), desc=\"Inference\"):\n",
    "            batch_end = min(batch_start + batch_size, len(test_dataset))\n",
    "            batch_examples = test_dataset[batch_start:batch_end]\n",
    "    \n",
    "            # Ensure batch_examples is properly formatted\n",
    "            if not isinstance(batch_examples[\"sample_id\"], list):\n",
    "                batch_examples = {k: [v] for k, v in batch_examples.items()}\n",
    "    \n",
    "            batch_sample_ids = batch_examples[\"sample_id\"]\n",
    "            batch_messages = batch_examples[\"messages\"]\n",
    "    \n",
    "            # Initialize batch containers\n",
    "            batch_texts = []\n",
    "            batch_images = []\n",
    "    \n",
    "            # Process all messages in a SINGLE loop (FIX: was doing two loops!)\n",
    "            for messages in batch_messages:\n",
    "                # Process text\n",
    "                text = processor.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                batch_texts.append(text)\n",
    "                \n",
    "                # Process images\n",
    "                images = []\n",
    "                for msg in messages:\n",
    "                    if isinstance(msg[\"content\"], list):\n",
    "                        for item in msg[\"content\"]:\n",
    "                            if isinstance(item, dict) and item.get(\"type\") == \"image\":\n",
    "                                try:\n",
    "                                    img = Image.open(item[\"image\"]).convert(\"RGB\")\n",
    "                                    images.append(img)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"‚ö†Ô∏è Error loading image {item['image']}: {e}\")\n",
    "                                    continue\n",
    "                \n",
    "                # Append images list (or None if no images)\n",
    "                batch_images.append(images if images else None)\n",
    "    \n",
    "            # Process inputs\n",
    "            inputs = processor(\n",
    "                text=batch_texts,\n",
    "                images=batch_images,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "            ).to(\"cuda\")\n",
    "    \n",
    "            # Generate predictions\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False,\n",
    "                    temperature=None,\n",
    "                    top_p=None,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                )\n",
    "    \n",
    "            # Decode predictions\n",
    "            for i, (in_ids, out_ids) in enumerate(zip(inputs.input_ids, generated_ids)):\n",
    "                generated_ids_trimmed = out_ids[len(in_ids):]\n",
    "                prediction = processor.decode(\n",
    "                    generated_ids_trimmed,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False,\n",
    "                ).strip()\n",
    "    \n",
    "                try:\n",
    "                    predicted_price = float(prediction)\n",
    "                except (ValueError, TypeError):\n",
    "                    print(\n",
    "                        f\"‚ö†Ô∏è Warning: Non-numeric prediction '{prediction}' for sample {batch_sample_ids[i]}, using 0.0\"\n",
    "                    )\n",
    "                    predicted_price = 0.0\n",
    "    \n",
    "                predictions.append(predicted_price)\n",
    "                sample_ids.append(batch_sample_ids[i])\n",
    "            \n",
    "            # Clean up batch memory\n",
    "            del inputs, generated_ids\n",
    "    \n",
    "            # Progress update\n",
    "            if batch_end % 500 == 0 or batch_end == len(test_dataset):\n",
    "                print(f\"  Processed {batch_end}/{len(test_dataset)} samples...\")\n",
    "    \n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({\"sample_id\": sample_ids, \"price\": predictions})\n",
    "        submission_path = \"test_out.csv\"\n",
    "    \n",
    "        # Validate submission format\n",
    "        print(\"\\n‚úÖ Validating submission format...\")\n",
    "        assert \"sample_id\" in submission_df.columns, \"Missing sample_id column\"\n",
    "        assert \"price\" in submission_df.columns, \"Missing price column\"\n",
    "        assert len(submission_df) == len(test_dataset), (\n",
    "            f\"Mismatch: {len(submission_df)} predictions vs {len(test_dataset)} test samples\"\n",
    "        )\n",
    "        assert submission_df[\"sample_id\"].duplicated().sum() == 0, (\n",
    "            \"Duplicate sample_ids found!\"\n",
    "        )\n",
    "        print(\"‚úÖ Submission format validated\")\n",
    "    \n",
    "        # Check for failed predictions\n",
    "        failed_predictions = sum(1 for p in predictions if p == 0.0)\n",
    "        if failed_predictions > len(predictions) * 0.1:\n",
    "            print(\n",
    "                f\"\\n‚ö†Ô∏è WARNING: {failed_predictions}/{len(predictions)} predictions failed (returned 0.0)\"\n",
    "            )\n",
    "            print(\"This suggests the model may need more training or better prompts\")\n",
    "    \n",
    "        # Save submission\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"‚úÖ Submission saved to: {submission_path}\")\n",
    "        print(f\"Total predictions: {len(predictions)}\")\n",
    "        print(f\"\\nSample predictions:\")\n",
    "        print(submission_df.head(10))\n",
    "        print(f\"{'=' * 60}\")\n",
    "    \n",
    "        # Final memory cleanup\n",
    "        del test_dataset, submission_df\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"\\n‚úÖ GPU memory cleaned up\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"[Rank {dist.get_rank()}] Skipping inference (only rank 0 runs inference)\")\n",
    "    \n",
    "    # CRITICAL: Sync all processes after inference completes\n",
    "    if is_distributed:\n",
    "        dist.barrier()\n",
    "        print(f\"[Rank {dist.get_rank() if dist.is_initialized() else 0}] ‚úÖ All processes synchronized after inference\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (t-2)",
   "language": "python",
   "name": "t-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
