{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c2db2-3343-4938-ac5a-c559ac45044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U transformers datasets peft trl bitsandbytes accelerate qwen-vl-utils pillow tensorboard webdataset unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9d1c2-bf8b-4f9c-b5b9-fa5bb907f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "from unsloth import FastVisionModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "import webdataset as wds\n",
    "from transformers import AutoProcessor, TrainerCallback, EarlyStoppingCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import IterableDataset\n",
    "\n",
    "try:\n",
    "    import unsloth_zoo.tokenizer_utils as tokenizer_utils_module\n",
    "    \n",
    "    # Replace the function with a no-op\n",
    "    def dummy_fix_untrained_tokens(*args, **kwargs):\n",
    "        print(\"‚úÖ Skipping fix_untrained_tokens (patched)\")\n",
    "        return None\n",
    "    \n",
    "    tokenizer_utils_module.fix_untrained_tokens = dummy_fix_untrained_tokens\n",
    "    print(\"‚úÖ Successfully patched unsloth_zoo.tokenizer_utils.fix_untrained_tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not patch tokenizer_utils: {e}\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" # <-- ADD THIS\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "# Initial cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01741cb-f8b8-4adf-ab61-7e1c40573e1b",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b80e67-1880-4773-96ed-b5dd203a32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN CONFIGURATION ---\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "OUTPUT_DIR = \"./qwen2.5-vl-3b-price-predictor-final\"\n",
    "\n",
    "# --- EXECUTION CONTROL ---\n",
    "# Set to True to enable evaluation, SMAPE, and early stopping\n",
    "USE_VALIDATION = False\n",
    "# Set to True to generate a submission file after training\n",
    "RUN_INFERENCE = True\n",
    "# Set to True to skip training and only run inference\n",
    "INFERENCE_ONLY = False\n",
    "# Set to True or a path to resume training from a checkpoint\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "\n",
    "# --- DATASET PATHS (WebDataset URL Patterns) ---\n",
    "# NOTE: Adjust the shard numbers (e.g., 000066) to match the output from 5_convert.py\n",
    "WEBDATASET_TRAIN_URL = \"./webdataset_train/train-shard-{000000..000067}.tar\"\n",
    "WEBDATASET_VALIDATION_URL = \"./webdataset_validation/validation-shard-{000000..000007}.tar\"\n",
    "WEBDATASET_TEST_URL = \"./webdataset_test/test-shard-{000000..000074}.tar\"\n",
    "\n",
    "\n",
    "# --- TRAINING HYPERPARAMETERS ---\n",
    "BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "NUM_WORKERS = 0\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "TRAIN_SAMPLES = 67499    # From preprocessing output\n",
    "STEPS_PER_EPOCH = TRAIN_SAMPLES // (BATCH_SIZE * GRADIENT_ACCUMULATION)\n",
    "MAX_STEPS = STEPS_PER_EPOCH * NUM_EPOCHS\n",
    "\n",
    "# --- EVALUATION SETTINGS ---\n",
    "# These are only active if USE_VALIDATION = True\n",
    "EVAL_STEPS = 250\n",
    "EARLY_STOPPING_PATIENCE = 3 # Stop after 3 evaluations with no improvement\n",
    "\n",
    "\n",
    "# --- PROCESSOR SETTINGS ---\n",
    "MIN_PIXELS = 256 * 28 * 28\n",
    "MAX_PIXELS = 512 * 28 * 28\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "if USE_VALIDATION:\n",
    "    print(f\"üöÄ Validation ENABLED: Evaluating every {EVAL_STEPS} steps.\")\n",
    "else:\n",
    "    print(\"üöÄ Validation DISABLED: Training on the full dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fde0e-9fd0-404a-a121-01e66d72a287",
   "metadata": {},
   "source": [
    "## Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9153ee-242c-46ed-86f5-64f1dd105072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Processor loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cef11-d4ee-4fcb-986f-dc9b370c5e62",
   "metadata": {},
   "source": [
    "## WebDataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383178f-b288-4c96-a455-d50fdd842e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sample(sample):\n",
    "    \"\"\"Decode preprocessed tensors from WebDataset using np.load to handle .npy format correctly\"\"\"\n",
    "    try:\n",
    "        # ‚úÖ CORRECT: Use np.load() with BytesIO to properly parse .npy format\n",
    "        # .npy files have headers - np.frombuffer() was reading EVERYTHING including header!\n",
    "        input_ids_array = np.load(io.BytesIO(sample[\"input_ids.npy\"]))\n",
    "        attention_mask_array = np.load(io.BytesIO(sample[\"attention_mask.npy\"]))\n",
    "        pixel_values_array = np.load(io.BytesIO(sample[\"pixel_values.npy\"]))\n",
    "        \n",
    "        metadata = json.loads(sample[\"metadata.json\"].decode(\"utf-8\"))\n",
    "        \n",
    "        # Now pixel_values_array has the CORRECT shape from the .npy file!\n",
    "        # It should be 2D: [num_patches, hidden_dim]\n",
    "        \n",
    "        if pixel_values_array.ndim != 2:\n",
    "            raise ValueError(f\"pixel_values has {pixel_values_array.ndim} dims, expected 2D [patches, hidden_dim]\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_ids = torch.from_numpy(input_ids_array).long()\n",
    "        attention_mask = torch.from_numpy(attention_mask_array).long()\n",
    "        pixel_values = torch.from_numpy(pixel_values_array).float()\n",
    "        \n",
    "        image_grid_thw = torch.tensor(metadata[\"image_grid_thw\"], dtype=torch.long)\n",
    "        \n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"image_grid_thw\": image_grid_thw,\n",
    "        }\n",
    "        \n",
    "        if \"sample_id\" in metadata:\n",
    "            result[\"sample_id\"] = metadata[\"sample_id\"]\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Let handler skip this corrupted sample\n",
    "        print(f\"‚ö†Ô∏è Skipping corrupted sample: {e}\")\n",
    "        raise  # Re-raise for wds.warn_and_continue handler\n",
    "\n",
    "\n",
    "def create_webdataset(url_pattern, is_train=True):\n",
    "    \"\"\"Create a WebDataset with proper decoding and error handling\"\"\"\n",
    "    dataset = (\n",
    "        wds.WebDataset(url_pattern, handler=wds.warn_and_continue)\n",
    "        .map(decode_sample, handler=wds.warn_and_continue)  # Handler skips errors\n",
    "    )\n",
    "    \n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "print(\"‚úÖ WebDataset functions defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397661f-a640-4a31-a266-bddb8c0dbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexableWebDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Wrapper to add __len__ and __getitem__ for Unsloth compatibility\"\"\"\n",
    "    def __init__(self, webdataset, length):\n",
    "        self.dataset = webdataset\n",
    "        self.length = length\n",
    "        self._cache = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Cache first 250 samples for Unsloth's fix_untrained_tokens check\"\"\"\n",
    "        if self._cache is None:\n",
    "            # Build cache on first access\n",
    "            self._cache = {}\n",
    "            iterator = iter(self.dataset)\n",
    "            for i in range(min(250, self.length)):\n",
    "                try:\n",
    "                    self._cache[i] = next(iterator)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        \n",
    "        if idx in self._cache:\n",
    "            return self._cache[idx]\n",
    "        \n",
    "        raise IndexError(f\"Index {idx} out of cacheable range (0-249)\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Forward actual training iteration to WebDataset\"\"\"\n",
    "        return iter(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319ee53-338f-40de-a29c-808fda267582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading WebDatasets...\")\n",
    "\n",
    "wds_train = create_webdataset(WEBDATASET_TRAIN_URL, is_train=True)\n",
    "\n",
    "# Wrap for Unsloth\n",
    "train_dataset = IndexableWebDataset(wds_train, length=TRAIN_SAMPLES)\n",
    "\n",
    "eval_dataset = create_webdataset(\n",
    "    WEBDATASET_VALIDATION_URL, \n",
    "    is_train=False\n",
    ") if USE_VALIDATION else None\n",
    "\n",
    "print(f\"‚úÖ Training dataset loaded from: {WEBDATASET_TRAIN_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfedaf-380a-4497-a9a6-62150b993570",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe25b1-a7f7-4153-8478-fe6836e4bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Loading Qwen2.5-VL-3B with Unsloth optimization...\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=MODEL_ID,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    use_gradient_checkpointing=False, # We found this was a bottleneck\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nüîß Fixing embedding size mismatch...\")\n",
    "print(f\"   Tokenizer vocab size: {len(processor.tokenizer)}\")\n",
    "print(f\"   Model vocab size BEFORE: {model.config.vocab_size}\")\n",
    "\n",
    "# Resize the embedding layer to handle ALL tokens (including vision tokens)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "print(f\"   Model vocab size AFTER: {model.config.vocab_size}\")\n",
    "print(\"‚úÖ Embedding layer resized! Vision tokens can now be processed.\")\n",
    "\n",
    "\n",
    "# model = FastVisionModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "#                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     bias=\"none\",\n",
    "#     use_gradient_checkpointing=False,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,      # Keep vision layers trainable\n",
    "    finetune_language_layers=True,     # Keep language layers trainable\n",
    "    finetune_attention_modules=True,   # Keep attention trainable\n",
    "    finetune_mlp_modules=True,         # Keep MLP trainable\n",
    "    r=16,                              # LoRA rank\n",
    "    lora_alpha=16,                     # LoRA alpha\n",
    "    lora_dropout=0.,                 # Dropout\n",
    "    bias=\"none\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# --- SFTConfig with Conditional Evaluation ---\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=EVAL_STEPS,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    dataloader_pin_memory=False,\n",
    "    torch_empty_cache_steps=20,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Add validation settings if using validation\n",
    "if USE_VALIDATION:\n",
    "    training_args.eval_strategy = \"steps\"  # ‚Üê CHANGE from \"epoch\"\n",
    "    training_args.eval_steps = EVAL_STEPS\n",
    "    training_args.metric_for_best_model = \"eval_loss\"  # Use loss, not SMAPE\n",
    "    training_args.greater_is_better = False\n",
    "    training_args.load_best_model_at_end = True\n",
    "\n",
    "print(f\"‚úÖ Training config: {NUM_EPOCHS} epochs, batch={BATCH_SIZE}, workers={NUM_WORKERS}\")\n",
    "print(\"‚úÖ Model loaded and SFTConfig configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23632505-a442-46e2-8210-d3216e724551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate with proper special token masking\"\"\"\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [example[\"input_ids\"] for example in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [example[\"attention_mask\"] for example in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    \n",
    "    pixel_values = torch.cat([example[\"pixel_values\"] for example in batch], dim=0)\n",
    "    image_grid_thw = torch.stack([example[\"image_grid_thw\"] for example in batch])\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # ‚úÖ THE KEY FIX: Mask ALL special tokens >= vocab_size\n",
    "    vocab_size = processor.tokenizer.vocab_size\n",
    "    labels[labels >= vocab_size] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"image_grid_thw\": image_grid_thw,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146af7c8-9eae-49e8-8401-edea2a979228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"Save checkpoint every N steps (overwrites previous)\"\"\"\n",
    "    def __init__(self, save_steps=60):\n",
    "        self.save_steps = save_steps\n",
    "        self.last_save_step = 0\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step - self.last_save_step >= self.save_steps:\n",
    "            checkpoint_dir = os.path.join(args.output_dir, \"checkpoint-periodic\")\n",
    "            kwargs[\"model\"].save_pretrained(checkpoint_dir)\n",
    "            print(f\"\\nüíæ Periodic checkpoint saved at step {state.global_step}\")\n",
    "            self.last_save_step = state.global_step\n",
    "        return control\n",
    "\n",
    "print(\"‚úÖ Periodic checkpoint callback defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e278175-3557-445b-a1fa-20ff59243cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    \"\"\"Clean up GPU memory between epochs\"\"\"\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        print(\"\\nüßπ GPU memory cleaned\")\n",
    "        return control\n",
    "\n",
    "if not INFERENCE_ONLY:\n",
    "    print(\"=\"*60)\n",
    "    print(\"INITIALIZING TRAINER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load validation dataset if enabled\n",
    "    eval_dataset_for_trainer = None\n",
    "    if USE_VALIDATION:\n",
    "        print(f\"Loading validation WebDataset from: {WEBDATASET_VALIDATION_URL}\")\n",
    "        eval_dataset_for_trainer = eval_dataset  # ‚Üê Use the one you already created!\n",
    "    \n",
    "    print(\"Loading trainer...\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # ‚Üê Plain IterableDataset (no wrapper!)\n",
    "        eval_dataset=eval_dataset_for_trainer,\n",
    "        data_collator=collate_fn,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.add_callback(MemoryCleanupCallback())\n",
    "    trainer.add_callback(PeriodicCheckpointCallback(save_steps=60))\n",
    "    \n",
    "    if USE_VALIDATION:\n",
    "        trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48db702-566c-4e64-8bc6-fc3fb9d61ff7",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c55fe-e7e8-47be-ab28-0f385fad9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Test a single batch to find the exact problem\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEBUGGING BATCH DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Get a single raw sample from the dataset\n",
    "print(\"\\nüì¶ Fetching one raw sample...\")\n",
    "raw_sample = next(iter(train_dataset))\n",
    "\n",
    "print(\"\\n‚úÖ Raw sample keys:\", raw_sample.keys())\n",
    "print(f\"   input_ids shape: {raw_sample['input_ids'].shape}\")\n",
    "print(f\"   pixel_values shape: {raw_sample['pixel_values'].shape}\")\n",
    "print(f\"   image_grid_thw shape: {raw_sample['image_grid_thw'].shape}\")\n",
    "\n",
    "# 2. Check raw input_ids values\n",
    "print(\"\\nüîç Raw input_ids statistics:\")\n",
    "print(f\"   Min: {raw_sample['input_ids'].min().item()}\")\n",
    "print(f\"   Max: {raw_sample['input_ids'].max().item()}\")\n",
    "print(f\"   Unique tokens: {torch.unique(raw_sample['input_ids']).numel()}\")\n",
    "\n",
    "# Find vision tokens\n",
    "vision_tokens = raw_sample['input_ids'][(raw_sample['input_ids'] >= 151650) & (raw_sample['input_ids'] <= 151660)]\n",
    "print(f\"   Vision tokens found: {torch.unique(vision_tokens).tolist()}\")\n",
    "\n",
    "# 3. Test collate_fn on a single-item batch\n",
    "print(\"\\nüîß Testing collate_fn with batch_size=1...\")\n",
    "batch = [raw_sample]\n",
    "try:\n",
    "    collated = collate_fn(batch)\n",
    "    print(\"   ‚úÖ Collation successful!\")\n",
    "    \n",
    "    print(\"\\nüìä Collated batch shapes:\")\n",
    "    print(f\"   input_ids: {collated['input_ids'].shape}\")\n",
    "    print(f\"   attention_mask: {collated['attention_mask'].shape}\")\n",
    "    print(f\"   pixel_values: {collated['pixel_values'].shape}\")\n",
    "    print(f\"   image_grid_thw: {collated['image_grid_thw'].shape}\")\n",
    "    print(f\"   labels: {collated['labels'].shape}\")\n",
    "    \n",
    "    print(\"\\nüîç Labels statistics:\")\n",
    "    print(f\"   Min (non-masked): {collated['labels'][collated['labels'] != -100].min().item()}\")\n",
    "    print(f\"   Max (non-masked): {collated['labels'][collated['labels'] != -100].max().item()}\")\n",
    "    print(f\"   Masked tokens (-100): {(collated['labels'] == -100).sum().item()}\")\n",
    "    print(f\"   Total tokens: {collated['labels'].numel()}\")\n",
    "    \n",
    "    # Check for vision tokens in labels (SHOULD BE -100!)\n",
    "    labels_flat = collated['labels'].view(-1)\n",
    "    vision_in_labels = labels_flat[(labels_flat >= 151650) & (labels_flat <= 151660) & (labels_flat != -100)]\n",
    "    if len(vision_in_labels) > 0:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Found {len(vision_in_labels)} vision tokens NOT masked!\")\n",
    "        print(f\"   Vision tokens in labels: {torch.unique(vision_in_labels).tolist()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All vision tokens properly masked\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Collation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. Check vocab size\n",
    "print(\"\\nüìö Model vocabulary:\")\n",
    "print(f\"   Tokenizer vocab_size: {processor.tokenizer.vocab_size}\")\n",
    "print(f\"   Model config vocab_size: {model.config.vocab_size}\")\n",
    "\n",
    "# Check if any labels exceed vocab size\n",
    "if 'collated' in locals():\n",
    "    labels_no_mask = collated['labels'][collated['labels'] != -100]\n",
    "    if len(labels_no_mask) > 0:\n",
    "        max_label = labels_no_mask.max().item()\n",
    "        if max_label >= processor.tokenizer.vocab_size:\n",
    "            print(f\"   ‚ùå PROBLEM: Max label {max_label} >= vocab_size {processor.tokenizer.vocab_size}\")\n",
    "            print(f\"   Out-of-bounds labels: {labels_no_mask[labels_no_mask >= processor.tokenizer.vocab_size].tolist()}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All labels within vocab range\")\n",
    "\n",
    "# 5. Test with multiple samples (batch_size=4)\n",
    "print(\"\\n\\nüß™ Testing with batch_size=4...\")\n",
    "batch_4 = []\n",
    "dataset_iter = iter(train_dataset)\n",
    "for i in range(4):\n",
    "    try:\n",
    "        batch_4.append(next(dataset_iter))\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "print(f\"   Collected {len(batch_4)} samples\")\n",
    "\n",
    "try:\n",
    "    collated_4 = collate_fn(batch_4)\n",
    "    print(\"   ‚úÖ Batch collation successful!\")\n",
    "    print(f\"   Batch shapes: input_ids={collated_4['input_ids'].shape}, labels={collated_4['labels'].shape}\")\n",
    "    \n",
    "    # Check labels again\n",
    "    labels_no_mask = collated_4['labels'][collated_4['labels'] != -100]\n",
    "    if len(labels_no_mask) > 0:\n",
    "        print(f\"   Labels range: {labels_no_mask.min().item()} to {labels_no_mask.max().item()}\")\n",
    "        if labels_no_mask.max().item() >= processor.tokenizer.vocab_size:\n",
    "            print(f\"   ‚ùå Found out-of-bounds labels!\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All labels valid\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Batch collation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUG COMPLETE - Share all output above!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717628cf-5ea4-4dd8-8506-35f239d24da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç INSPECTING RAW INPUT_IDS FROM WEBDATASET\")\n",
    "raw_sample = next(iter(train_dataset))\n",
    "\n",
    "# Get the actual token IDs\n",
    "token_ids = raw_sample['input_ids']\n",
    "\n",
    "print(f\"\\nToken IDs shape: {token_ids.shape}\")\n",
    "print(f\"Min ID: {token_ids.min().item()}\")\n",
    "print(f\"Max ID: {token_ids.max().item()}\")\n",
    "\n",
    "# Find the HUGE values\n",
    "huge_values = token_ids[token_ids > 151660]\n",
    "if len(huge_values) > 0:\n",
    "    print(f\"\\n‚ùå PROBLEM FOUND!\")\n",
    "    print(f\"   Found {len(huge_values)} tokens with IDs > 151660\")\n",
    "    print(f\"   These values: {huge_values.tolist()[:20]}\")  # Show first 20\n",
    "    print(f\"\\n   Positions: {torch.where(token_ids > 151660)[0].tolist()[:10]}\")\n",
    "    \n",
    "    # Show what's around them\n",
    "    for pos in torch.where(token_ids > 151660)[0][:3]:\n",
    "        start = max(0, pos-5)\n",
    "        end = min(len(token_ids), pos+6)\n",
    "        print(f\"\\n   Context around position {pos}:\")\n",
    "        print(f\"   {token_ids[start:end].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712eb083-5975-4dcb-9a8e-c30b91e2d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç CHECKING image_grid_thw VALUES\")\n",
    "for i, sample in enumerate(iter(train_dataset)):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"   image_grid_thw: {sample['image_grid_thw']}\")\n",
    "    print(f\"   pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "    \n",
    "    t, h, w = sample['image_grid_thw']\n",
    "    expected = t * h * w\n",
    "    actual = sample['pixel_values'].shape[0]\n",
    "    \n",
    "    print(f\"   Expected patches (t*h*w): {expected}\")\n",
    "    print(f\"   Actual patches: {actual}\")\n",
    "    print(f\"   Match: {expected == actual}\")\n",
    "    \n",
    "    if expected != actual:\n",
    "        print(f\"   ‚ùå MISMATCH FOUND!\")\n",
    "        break\n",
    "    \n",
    "    if i >= 4:  # Check first 5 samples\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31b83d-c846-442b-a406-7489314c316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INFERENCE_ONLY:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    \n",
    "    # When load_best_model_at_end=True, the trainer already holds the best model.\n",
    "    # We just need to save it.\n",
    "    if USE_VALIDATION:\n",
    "        print(f\"Saving the best model from checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Saving final model...\")\n",
    "        \n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"‚úÖ Model and processor saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b1194-ca1a-4c0c-af71-3560ce27f1f6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd64873-4843-44e5-8952-87bf3820e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INFERENCE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING INFERENCE ON TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get the model\n",
    "    inference_model = trainer.model if hasattr(trainer, 'model') else model\n",
    "    inference_model.eval()\n",
    "    \n",
    "    # Load test WebDataset\n",
    "    test_wds = create_webdataset(WEBDATASET_TEST_URL, is_train=False)\n",
    "    \n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "    batch = []\n",
    "    \n",
    "    print(\"Processing test samples...\")\n",
    "    \n",
    "    for sample in tqdm(test_wds, desc=\"Inference\", total=75000):\n",
    "        batch.append(sample)\n",
    "        \n",
    "        # Process batch when full\n",
    "        if len(batch) == EVAL_BATCH_SIZE:\n",
    "            # Use collate_fn to prepare batch (already preprocessed!)\n",
    "            batch_data = collate_fn(batch)\n",
    "            \n",
    "            # Move to GPU (remove labels for inference)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch_data[\"input_ids\"].to(\"cuda\"),\n",
    "                \"attention_mask\": batch_data[\"attention_mask\"].to(\"cuda\"),\n",
    "                \"pixel_values\": batch_data[\"pixel_values\"].to(\"cuda\"),\n",
    "                \"image_grid_thw\": batch_data[\"image_grid_thw\"].to(\"cuda\"),\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_ids = inference_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            # Decode only the generated part\n",
    "            for i, (in_ids, out_ids) in enumerate(zip(inputs[\"input_ids\"], generated_ids)):\n",
    "                generated_ids_trimmed = out_ids[len(in_ids):]\n",
    "                prediction = processor.decode(\n",
    "                    generated_ids_trimmed,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False,\n",
    "                ).strip()\n",
    "                \n",
    "                # Extract price\n",
    "                try:\n",
    "                    predicted_price = float(prediction)\n",
    "                except:\n",
    "                    predicted_price = 0.0\n",
    "                \n",
    "                predictions.append(predicted_price)\n",
    "                sample_ids.append(batch[i][\"sample_id\"])\n",
    "            \n",
    "            # Clear batch\n",
    "            batch = []\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del inputs, generated_ids\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Handle remaining samples\n",
    "    if len(batch) > 0:\n",
    "        batch_data = collate_fn(batch)\n",
    "        inputs = {\n",
    "            \"input_ids\": batch_data[\"input_ids\"].to(\"cuda\"),\n",
    "            \"attention_mask\": batch_data[\"attention_mask\"].to(\"cuda\"),\n",
    "            \"pixel_values\": batch_data[\"pixel_values\"].to(\"cuda\"),\n",
    "            \"image_grid_thw\": batch_data[\"image_grid_thw\"].to(\"cuda\"),\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = inference_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        for i, (in_ids, out_ids) in enumerate(zip(inputs[\"input_ids\"], generated_ids)):\n",
    "            generated_ids_trimmed = out_ids[len(in_ids):]\n",
    "            prediction = processor.decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            ).strip()\n",
    "            \n",
    "            try:\n",
    "                predicted_price = float(prediction)\n",
    "            except:\n",
    "                predicted_price = 0.0\n",
    "            \n",
    "            predictions.append(predicted_price)\n",
    "            sample_ids.append(batch[i][\"sample_id\"])\n",
    "    \n",
    "    # Create submission\n",
    "    submission_df = pd.DataFrame({\"sample_id\": sample_ids, \"price\": predictions})\n",
    "    submission_df.to_csv(\"test_out.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inference complete!\")\n",
    "    print(f\"   Total predictions: {len(predictions):,}\")\n",
    "    print(f\"   Saved to: test_out.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e9336-fad8-4416-94c6-9cf499ffea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# POST-PROCESSING: FILL MISSING PREDICTIONS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING FOR MISSING PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load test CSV and submission\n",
    "test_csv = pd.read_csv(\"../dataset/test.csv\")\n",
    "submission_df = pd.read_csv(\"test_out.csv\")\n",
    "\n",
    "# Find missing sample IDs\n",
    "all_test_ids = set(test_csv[\"sample_id\"].tolist())\n",
    "predicted_ids = set(submission_df[\"sample_id\"].tolist())\n",
    "missing_ids = sorted(all_test_ids - predicted_ids)\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"‚ö†Ô∏è  Found {len(missing_ids)} missing predictions\")\n",
    "    print(f\"   Sample IDs: {missing_ids}\")\n",
    "    \n",
    "    # Use training data mean as fallback\n",
    "    train_csv = pd.read_csv(\"../dataset/train.csv\")\n",
    "    mean_price = train_csv[\"price\"].mean()\n",
    "    median_price = train_csv[\"price\"].median()\n",
    "    \n",
    "    print(f\"\\nüí° Fallback strategy:\")\n",
    "    print(f\"   Training mean price: ${mean_price:.2f}\")\n",
    "    print(f\"   Training median price: ${median_price:.2f}\")\n",
    "    print(f\"   Using MEDIAN (more robust to outliers)\")\n",
    "    \n",
    "    # Create missing predictions with median\n",
    "    missing_rows = pd.DataFrame({\n",
    "        \"sample_id\": missing_ids,\n",
    "        \"price\": [median_price] * len(missing_ids)\n",
    "    })\n",
    "    \n",
    "    # Add to submission\n",
    "    submission_df = pd.concat([submission_df, missing_rows], ignore_index=True)\n",
    "    submission_df = submission_df.sort_values(\"sample_id\").reset_index(drop=True)\n",
    "    submission_df.to_csv(\"test_out.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Added {len(missing_ids)} predictions with median price: ${median_price:.2f}\")\n",
    "    print(f\"   Impact: {len(missing_ids)/75000*100:.4f}% of test set\")\n",
    "else:\n",
    "    print(\"‚úÖ All 75,000 predictions present!\")\n",
    "\n",
    "# Verify final count\n",
    "print(f\"\\nüìä Final submission stats:\")\n",
    "print(f\"   Total rows: {len(submission_df)}\")\n",
    "print(f\"   Sample ID range: {submission_df['sample_id'].min()} - {submission_df['sample_id'].max()}\")\n",
    "print(f\"   Price range: ${submission_df['price'].min():.2f} - ${submission_df['price'].max():.2f}\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (t-2)",
   "language": "python",
   "name": "t-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
