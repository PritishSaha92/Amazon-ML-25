{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9006034",
   "metadata": {},
   "source": [
    "##  Basic Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U transformers datasets peft trl bitsandbytes accelerate qwen-vl-utils pillow tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432e4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "import torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATASET_FOLDER = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ae1170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Full Training Mode: False\n",
      "  Run Inference: False\n",
      "  Skip Dataset Creation: False\n",
      "  Skip Image Download: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndevelopment (validation mode):\\nFULL_TRAINING_MODE = False\\nINFERENCE_ONLY = False\\nRUN_INFERENCE = False\\nSKIP_DATASET_CREATION = False\\nSKIP_IMAGE_DOWNLOAD = False\\n\\nfull training:\\nFULL_TRAINING_MODE = True   # Use all data\\nINFERENCE_ONLY = False\\nRUN_INFERENCE = False\\nSKIP_DATASET_CREATION = True   # Reuse\\nSKIP_IMAGE_DOWNLOAD = True     # Reuse\\n\\ngenerate submission:\\nFULL_TRAINING_MODE = True   # Doesn't matter\\nINFERENCE_ONLY = True       # Skip training!\\nRUN_INFERENCE = True        # Generate predictions\\nSKIP_DATASET_CREATION = True\\nSKIP_IMAGE_DOWNLOAD = True\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FULL_TRAINING_MODE = False\n",
    "\n",
    "# Set to True to run inference on test set and generate submission\n",
    "RUN_INFERENCE = False\n",
    "\n",
    "# Set to True to skip dataset creation if already created\n",
    "SKIP_DATASET_CREATION = False\n",
    "\n",
    "# Set to True to skip image downloading if already done\n",
    "SKIP_IMAGE_DOWNLOAD = False\n",
    "\n",
    "INFERENCE_CHECKPOINT = None  # None = use default output_dir\n",
    "\n",
    "INFERENCE_ONLY = False\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "\n",
    "\n",
    "# Options for resuming:\n",
    "# RESUME_FROM_CHECKPOINT = \"./qwen2-vl-7b-price-predictor-best/periodic_checkpoint\"  # Latest periodic\n",
    "# RESUME_FROM_CHECKPOINT = \"./qwen2-vl-7b-price-predictor-best/checkpoint-epoch-1\"  # Specific epoch\n",
    "# RESUME_FROM_CHECKPOINT = True  # Auto-detect latest checkpoint\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Full Training Mode: {FULL_TRAINING_MODE}\")\n",
    "print(f\"  Run Inference: {RUN_INFERENCE}\")\n",
    "print(f\"  Skip Dataset Creation: {SKIP_DATASET_CREATION}\")\n",
    "print(f\"  Skip Image Download: {SKIP_IMAGE_DOWNLOAD}\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "development (validation mode):\n",
    "FULL_TRAINING_MODE = False\n",
    "INFERENCE_ONLY = False\n",
    "RUN_INFERENCE = False\n",
    "SKIP_DATASET_CREATION = False\n",
    "SKIP_IMAGE_DOWNLOAD = False\n",
    "\n",
    "full training:\n",
    "FULL_TRAINING_MODE = True   # Use all data\n",
    "INFERENCE_ONLY = False\n",
    "RUN_INFERENCE = False\n",
    "SKIP_DATASET_CREATION = True   # Reuse\n",
    "SKIP_IMAGE_DOWNLOAD = True     # Reuse\n",
    "\n",
    "generate submission:\n",
    "FULL_TRAINING_MODE = True   # Doesn't matter\n",
    "INFERENCE_ONLY = True       # Skip training!\n",
    "RUN_INFERENCE = True        # Generate predictions\n",
    "SKIP_DATASET_CREATION = True\n",
    "SKIP_IMAGE_DOWNLOAD = True\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8811f03",
   "metadata": {},
   "source": [
    "##  Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cad5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 37848/75000 [00:01<00:01, 29379.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/51mjZYDYjyL.jpg\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:02<00:00, 29289.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded images for 75000 training samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import download_images\n",
    "\n",
    "if not SKIP_IMAGE_DOWNLOAD:\n",
    "    print(\"Downloading training images...\")\n",
    "    train = pd.read_csv(os.path.join(DATASET_FOLDER, \"train.csv\"))\n",
    "    download_images(train[\"image_link\"], f\"{DATASET_FOLDER}/train_images\")\n",
    "    print(f\"Downloaded images for {len(train)} training samples\")\n",
    "else:\n",
    "    print(\"Skipping image download (SKIP_IMAGE_DOWNLOAD=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae4e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 41035/75000 [00:01<00:01, 29289.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:02<00:00, 28978.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded images for 75000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_IMAGE_DOWNLOAD:\n",
    "    print(\"Downloading test images...\")\n",
    "    test = pd.read_csv(os.path.join(DATASET_FOLDER, \"test.csv\"))\n",
    "    download_images(test[\"image_link\"], f\"{DATASET_FOLDER}/test_images\")\n",
    "    print(f\"Downloaded images for {len(test)} test samples\")\n",
    "else:\n",
    "    print(\"Skipping test image download (SKIP_IMAGE_DOWNLOAD=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbd51d-b1a6-468c-8ac9-8a7e3a37d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "# Enable loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def resize_single_image(filename, input_folder, output_folder, max_size):\n",
    "    \"\"\"Resize a single image (for parallel processing)\"\"\"\n",
    "    input_path = os.path.join(input_folder, filename)\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    \n",
    "    # Skip if already resized\n",
    "    if os.path.exists(output_path):\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(input_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize maintaining aspect ratio\n",
    "        img.thumbnail((max_size, max_size), Image.LANCZOS)\n",
    "        \n",
    "        # Save with good quality\n",
    "        img.save(output_path, \"JPEG\", quality=95, optimize=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # If resize fails, copy original\n",
    "        try:\n",
    "            shutil.copy(input_path, output_path)\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "def resize_images_in_folder(input_folder, output_folder, max_size=512, num_workers=16):\n",
    "    \"\"\"\n",
    "    Pre-resizes all images using multiprocessing (10x faster!).\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Source folder with original images\n",
    "        output_folder: Destination folder for resized images\n",
    "        max_size: Maximum dimension (width or height)\n",
    "        num_workers: Number of parallel workers\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    print(f\"Resizing {len(image_files)} images from {input_folder}...\")\n",
    "    print(f\"Using {num_workers} parallel workers\")\n",
    "    \n",
    "    # Create partial function with fixed arguments\n",
    "    resize_partial = partial(\n",
    "        resize_single_image,\n",
    "        input_folder=input_folder,\n",
    "        output_folder=output_folder,\n",
    "        max_size=max_size\n",
    "    )\n",
    "    \n",
    "    # Use multiprocessing pool\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(resize_partial, image_files),\n",
    "            total=len(image_files),\n",
    "            desc=\"Resizing\"\n",
    "        ))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "    failed_count = len([r for r in results if not r])\n",
    "    \n",
    "    print(f\"✅ All images resized and saved to {output_folder}\")\n",
    "    if failed_count > 0:\n",
    "        print(f\"⚠️  {failed_count} images had issues (copied originals)\")\n",
    "\n",
    "# Resize images after downloading\n",
    "if not SKIP_IMAGE_DOWNLOAD:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESIZING IMAGES FOR FASTER TRAINING\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Resize training images (with 16 workers!)\n",
    "    resize_images_in_folder(\n",
    "        input_folder=f\"{DATASET_FOLDER}/train_images\",\n",
    "        output_folder=f\"{DATASET_FOLDER}/train_images_resized\",\n",
    "        max_size=512,\n",
    "        num_workers=16  # Parallel processing!\n",
    "    )\n",
    "    \n",
    "    # Resize test images\n",
    "    resize_images_in_folder(\n",
    "        input_folder=f\"{DATASET_FOLDER}/test_images\",\n",
    "        output_folder=f\"{DATASET_FOLDER}/test_images_resized\",\n",
    "        max_size=512,\n",
    "        num_workers=16\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Image resizing complete!\")\n",
    "else:\n",
    "    print(\"Skipping image resizing (SKIP_IMAGE_DOWNLOAD=True)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb33cd8",
   "metadata": {},
   "source": [
    "## Create JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts filename from image URL.\n",
    "\n",
    "    Example:\n",
    "    'https://m.media-amazon.com/images/I/51mjZYDYjyL.jpg'\n",
    "    -> '51mjZYDYjyL.jpg'\n",
    "    \"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        return None\n",
    "    try:\n",
    "        path = urlparse(url).path\n",
    "        filename = os.path.basename(path)\n",
    "        return filename\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ce2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jsonl_dataset(csv_path, image_folder, output_path, is_test_set=False):\n",
    "    \"\"\"\n",
    "    Converts CSV + local images to JSONL format for Qwen2.5-VL.\n",
    "\n",
    "    Dataset Format:\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"system prompt\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": \"file:///absolute/path.jpg\"},\n",
    "                {\"type\": \"text\", \"text\": \"user prompt\"}\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": \"price\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Key Points:\n",
    "    - Uses file:/// (3 slashes) for local absolute paths\n",
    "    - Content can be string OR list of dicts for multimodal\n",
    "    - Assistant response only included for training set\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Processing {len(df)} rows from {os.path.basename(csv_path)}...\")\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            image_filename = get_filename_from_url(row[\"image_link\"])\n",
    "            if not image_filename:\n",
    "                print(\n",
    "                    f\"Warning: Could not parse filename from URL for sample_id {row['sample_id']}. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            resized_folder = image_folder + \"_resized\"\n",
    "            if os.path.exists(resized_folder):\n",
    "                image_path = os.path.abspath(os.path.join(resized_folder, image_filename))\n",
    "            else:\n",
    "                image_path = os.path.abspath(os.path.join(image_folder, image_filename))\n",
    "            image_exists = os.path.exists(image_path)\n",
    "\n",
    "            if not image_exists and not is_test_set:\n",
    "                print(\n",
    "                    f\"Skipping training sample_id {row['sample_id']}: Image not found at {image_path}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            product_description = row[\"catalog_content\"].strip()\n",
    "\n",
    "            system_prompt = (\n",
    "                \"You are a precise e-commerce expert. Your task is to analyze the product \"\n",
    "                \"image and its description, then output only the product's price as a numerical \"\n",
    "                \"value. Do not add any other text or currency symbols.\"\n",
    "            )\n",
    "\n",
    "            user_prompt_text = (\n",
    "                f\"Analyze the following product and provide its price.\\n\\n\"\n",
    "                f\"**Product Information:**\\n{product_description}\"\n",
    "            )\n",
    "\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "            if image_exists:\n",
    "                user_content = [\n",
    "                    {\"type\": \"image\", \"image\": image_path},\n",
    "                    {\"type\": \"text\", \"text\": user_prompt_text},\n",
    "                ]\n",
    "            else:\n",
    "                print(f\"Creating text-only entry for test sample_id {row['sample_id']}\")\n",
    "                user_content = [{\"type\": \"text\", \"text\": user_prompt_text}]\n",
    "\n",
    "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "            if not is_test_set:\n",
    "                price_float = float(row[\"price\"])\n",
    "                price = f\"{round(price_float, 2):.2f}\"\n",
    "                messages.append({\"role\": \"assistant\", \"content\": price})\n",
    "\n",
    "            jsonl_entry = {\n",
    "                \"sample_id\": row[\"sample_id\"],\n",
    "                \"messages\": messages,\n",
    "            }\n",
    "            f.write(json.dumps(jsonl_entry) + \"\\n\")\n",
    "\n",
    "    print(f\"Successfully created dataset at {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06004857",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_DATASET_CREATION:\n",
    "    create_jsonl_dataset(\n",
    "        csv_path=os.path.join(DATASET_FOLDER, \"train.csv\"),\n",
    "        image_folder=f\"{DATASET_FOLDER}/train_images\",\n",
    "        output_path=f\"{DATASET_FOLDER}/train.jsonl\",\n",
    "        is_test_set=False,\n",
    "    )\n",
    "\n",
    "    create_jsonl_dataset(\n",
    "        csv_path=os.path.join(DATASET_FOLDER, \"test.csv\"),\n",
    "        image_folder=f\"{DATASET_FOLDER}/test_images\",\n",
    "        output_path=f\"{DATASET_FOLDER}/test.jsonl\",\n",
    "        is_test_set=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping dataset creation (SKIP_DATASET_CREATION=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3db72-a008-4ffd-9ca5-b61ccd10cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def count_images_in_folder(folder_path):\n",
    "    \"\"\"Count total image files in a folder\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        return 0\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    return len(image_files)\n",
    "\n",
    "def count_unique_images_in_csv(csv_path):\n",
    "    \"\"\"Count unique image URLs in CSV\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df['image_link'].nunique()\n",
    "\n",
    "def count_jsonl_entries(jsonl_path):\n",
    "    \"\"\"Count total entries in JSONL file\"\"\"\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        return 0\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def count_jsonl_with_images(jsonl_path):\n",
    "    \"\"\"Count JSONL entries that have images\"\"\"\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        return 0\n",
    "    count = 0\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Check if any message content has an image\n",
    "            for msg in data.get('messages', []):\n",
    "                if isinstance(msg.get('content'), list):\n",
    "                    for item in msg['content']:\n",
    "                        if isinstance(item, dict) and item.get('type') == 'image':\n",
    "                            count += 1\n",
    "                            break\n",
    "    return count\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET STATISTICS & VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# TRAIN DATASET\n",
    "print(\"\\n📊 TRAINING DATASET:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "train_csv_rows = len(pd.read_csv(f\"{DATASET_FOLDER}/train.csv\"))\n",
    "train_unique_urls = count_unique_images_in_csv(f\"{DATASET_FOLDER}/train.csv\")\n",
    "train_images = count_images_in_folder(f\"{DATASET_FOLDER}/train_images\")\n",
    "train_images_resized = count_images_in_folder(f\"{DATASET_FOLDER}/train_images_resized\")\n",
    "train_jsonl_entries = count_jsonl_entries(f\"{DATASET_FOLDER}/train.jsonl\")\n",
    "train_jsonl_with_images = count_jsonl_with_images(f\"{DATASET_FOLDER}/train.jsonl\")\n",
    "\n",
    "print(f\"  CSV rows:                    {train_csv_rows:>6,}\")\n",
    "print(f\"  Unique image URLs in CSV:    {train_unique_urls:>6,}\")\n",
    "print(f\"  Downloaded images:           {train_images:>6,}\")\n",
    "print(f\"  Resized images:              {train_images_resized:>6,}\")\n",
    "print(f\"  JSONL total entries:         {train_jsonl_entries:>6,}\")\n",
    "print(f\"  JSONL entries with images:   {train_jsonl_with_images:>6,}\")\n",
    "print(f\"  JSONL entries text-only:     {train_jsonl_entries - train_jsonl_with_images:>6,}\")\n",
    "\n",
    "# Calculate differences\n",
    "train_missing_downloads = train_unique_urls - train_images\n",
    "train_missing_resized = train_images - train_images_resized\n",
    "train_skipped_jsonl = train_csv_rows - train_jsonl_entries\n",
    "\n",
    "print(f\"\\n  Missing from download:       {train_missing_downloads:>6,} ({train_missing_downloads/train_unique_urls*100:.2f}%)\")\n",
    "print(f\"  Failed to resize:            {train_missing_resized:>6,}\")\n",
    "print(f\"  Skipped in JSONL:            {train_skipped_jsonl:>6,}\")\n",
    "\n",
    "# TEST DATASET\n",
    "print(\"\\n📊 TEST DATASET:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "test_csv_rows = len(pd.read_csv(f\"{DATASET_FOLDER}/test.csv\"))\n",
    "test_unique_urls = count_unique_images_in_csv(f\"{DATASET_FOLDER}/test.csv\")\n",
    "test_images = count_images_in_folder(f\"{DATASET_FOLDER}/test_images\")\n",
    "test_images_resized = count_images_in_folder(f\"{DATASET_FOLDER}/test_images_resized\")\n",
    "test_jsonl_entries = count_jsonl_entries(f\"{DATASET_FOLDER}/test.jsonl\")\n",
    "test_jsonl_with_images = count_jsonl_with_images(f\"{DATASET_FOLDER}/test.jsonl\")\n",
    "\n",
    "print(f\"  CSV rows:                    {test_csv_rows:>6,}\")\n",
    "print(f\"  Unique image URLs in CSV:    {test_unique_urls:>6,}\")\n",
    "print(f\"  Downloaded images:           {test_images:>6,}\")\n",
    "print(f\"  Resized images:              {test_images_resized:>6,}\")\n",
    "print(f\"  JSONL total entries:         {test_jsonl_entries:>6,}\")\n",
    "print(f\"  JSONL entries with images:   {test_jsonl_with_images:>6,}\")\n",
    "print(f\"  JSONL entries text-only:     {test_jsonl_entries - test_jsonl_with_images:>6,}\")\n",
    "\n",
    "# Calculate differences\n",
    "test_missing_downloads = test_unique_urls - test_images\n",
    "test_missing_resized = test_images - test_images_resized\n",
    "test_text_only = test_jsonl_entries - test_jsonl_with_images\n",
    "\n",
    "print(f\"\\n  Missing from download:       {test_missing_downloads:>6,} ({test_missing_downloads/test_unique_urls*100:.2f}%)\")\n",
    "print(f\"  Failed to resize:            {test_missing_resized:>6,}\")\n",
    "print(f\"  Text-only in JSONL:          {test_text_only:>6,} (for missing images)\")\n",
    "\n",
    "# SUMMARY\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Total training samples:      {train_jsonl_with_images:>6,} ✅\")\n",
    "print(f\"  Total test samples:          {test_jsonl_entries:>6,} ({test_jsonl_with_images:,} with images)\")\n",
    "print(f\"  Ready for training:          {'YES ✅' if train_jsonl_with_images > 60000 else 'NO ❌'}\")\n",
    "print(f\"  Ready for inference:         {'YES ✅' if test_jsonl_entries == 75000 else 'NO ❌'}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "print(\"Loading training data from JSONL...\")\n",
    "train_data_raw = []\n",
    "with open(f\"{DATASET_FOLDER}/train.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        try:\n",
    "            train_data_raw.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Skipping malformed JSON on line {line_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"✅ Loaded {len(train_data_raw)} training samples\")\n",
    "\n",
    "\n",
    "if FULL_TRAINING_MODE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL TRAINING MODE: Using entire dataset for training\")\n",
    "    print(\"=\" * 60)\n",
    "    train_dataset = train_data_raw\n",
    "    validation_dataset = None\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(\"Validation: None (full training mode)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATION MODE: Splitting dataset for SMAPE monitoring\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Manual 90/10 split with shuffling\n",
    "    random.seed(42)\n",
    "    random.shuffle(train_data_raw)\n",
    "    \n",
    "    split_idx = int(len(train_data_raw) * 0.9)\n",
    "    train_dataset = train_data_raw[:split_idx]\n",
    "    validation_dataset = train_data_raw[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(validation_dataset)}\")\n",
    "\n",
    "print(\"\\n✅ Datasets ready for training!\")\n",
    "print(f\"Dataset type: {type(train_dataset)}\")  # Will be <class 'list'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591f7fa",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from unsloth import FastVisionModel\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "train_dataset_path = \"./train_split.jsonl\"\n",
    "validation_dataset_path = \"./validation_split.jsonl\"\n",
    "output_dir = \"./qwen2.5-vl-3b-price-predictor-best\"\n",
    "\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class SMAPEEvaluationCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Computes SMAPE after each epoch on validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, trainer, validation_dataset, processor, device=\"cuda\", max_samples=500\n",
    "    ):\n",
    "        self.trainer = trainer\n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.max_samples = max_samples\n",
    "        self.best_smape = float(\"inf\")\n",
    "        self.smape_history = []\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at end of each epoch.\"\"\"\n",
    "        if self.validation_dataset is None:\n",
    "            print(\n",
    "                \"\\nSkipping SMAPE evaluation (no validation set in FULL_TRAINING_MODE)\"\n",
    "            )\n",
    "            return control\n",
    "        try:\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"Computing SMAPE for Epoch {int(state.epoch)}...\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "\n",
    "            self.trainer.model.eval()\n",
    "            smape_scores = []\n",
    "\n",
    "            num_samples = min(len(self.validation_dataset), self.max_samples)\n",
    "            validation_subset = self.validation_dataset[:num_samples]\n",
    "\n",
    "            for idx, example in enumerate(validation_subset):\n",
    "                messages = example[\"messages\"][:-1]\n",
    "                ground_truth = example[\"messages\"][-1][\"content\"]\n",
    "\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                images = []\n",
    "                for msg in messages:\n",
    "                    if isinstance(msg[\"content\"], list):\n",
    "                        for item in msg[\"content\"]:\n",
    "                            if isinstance(item, dict) and item.get(\"type\") == \"image\":\n",
    "                                images.append(item[\"image\"])\n",
    "\n",
    "                inputs = self.processor(\n",
    "                    text=[text],\n",
    "                    images=[images] if images else None,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ).to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.trainer.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=20,\n",
    "                        num_beams=1,\n",
    "                        do_sample=False,\n",
    "                        temperature=None,\n",
    "                        top_p=None,\n",
    "                        pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=self.processor.tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                generated_ids_trimmed = [\n",
    "                    out_ids[len(in_ids) :]\n",
    "                    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                ]\n",
    "                prediction = self.processor.batch_decode(\n",
    "                    generated_ids_trimmed,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False,\n",
    "                )[0].strip()\n",
    "\n",
    "                try:\n",
    "                    pred_price = float(prediction)\n",
    "                    true_price = float(ground_truth)\n",
    "\n",
    "                    numerator = abs(pred_price - true_price)\n",
    "                    denominator = (abs(true_price) + abs(pred_price)) / 2\n",
    "\n",
    "                    if denominator == 0:\n",
    "                        smape = 0.0 if numerator == 0 else 100.0\n",
    "                    else:\n",
    "                        smape = (numerator / denominator) * 100\n",
    "\n",
    "                    smape_scores.append(smape)\n",
    "\n",
    "                except (ValueError, TypeError):\n",
    "                    smape_scores.append(100.0)\n",
    "\n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    current_avg = np.mean(smape_scores)\n",
    "                    print(f\"  [{idx + 1}/{num_samples}] Running SMAPE: {current_avg:.2f}%\")\n",
    "\n",
    "            avg_smape = np.mean(smape_scores)\n",
    "            self.smape_history.append(avg_smape)\n",
    "\n",
    "            logs = {\"eval_smape\": avg_smape}\n",
    "            self.trainer.log(logs)\n",
    "\n",
    "            if avg_smape < self.best_smape:\n",
    "                self.best_smape = avg_smape\n",
    "                print(f\"\\n✨ NEW BEST SMAPE: {avg_smape:.2f}%\")\n",
    "\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"Epoch {int(state.epoch)} SMAPE: {avg_smape:.2f}%\")\n",
    "            print(f\"Best SMAPE: {self.best_smape:.2f}%\")\n",
    "            print(f\"Target: < 44% (leaderboard #1)\")\n",
    "            print(f\"{'=' * 60}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during SMAPE evaluation: {e}\")\n",
    "            print(\"Continuing training despite SMAPE error...\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            return control\n",
    "\n",
    "        self.trainer.model.train()\n",
    "        return control\n",
    "\n",
    "\n",
    "print(\"SMAPE Callback defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ce7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Early stopping based on SMAPE metric.\n",
    "    Stops training if SMAPE doesn't improve for N epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smape_callback, patience=2):\n",
    "        self.smape_callback = smape_callback\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "        self.best_smape = float(\"inf\")\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.smape_callback is None:\n",
    "            return control\n",
    "\n",
    "        current_smape = (\n",
    "            self.smape_callback.smape_history[-1]\n",
    "            if self.smape_callback.smape_history\n",
    "            else float(\"inf\")\n",
    "        )\n",
    "\n",
    "        if current_smape < self.best_smape:\n",
    "            self.best_smape = current_smape\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            print(f\"***  No improvement for {self.patience_counter} epoch(s)\")\n",
    "\n",
    "        if self.patience_counter >= self.patience:\n",
    "            print(\n",
    "                f\"\\n*** Early stopping triggered! No improvement for {self.patience} epochs.\"\n",
    "            )\n",
    "            control.should_training_stop = True\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bfad0-a46e-46f5-930d-eefcdeec4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Loading Qwen2.5-VL-3B with Unsloth optimization...\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=model_id,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    # use_gradient_checkpointing=True,\n",
    "    use_gradient_checkpointing=False,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Enable training mode\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    # use_gradient_checkpointing=\"unsloth\",\n",
    "    use_gradient_checkpointing=False,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading processor...\")\n",
    "min_pix = 256 * 28 * 28\n",
    "max_pix = 512 * 28 * 28\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, min_pixels=min_pix, max_pixels=max_pix, trust_remote_code=True)\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "print(f\"   EOS token: '{processor.tokenizer.eos_token}' (ID: {processor.tokenizer.eos_token_id})\")\n",
    "print(f\"   PAD token: '{processor.tokenizer.pad_token}' (ID: {processor.tokenizer.pad_token_id})\")\n",
    "print(\"Processor loaded successfully!\")\n",
    "print(f\"Vocab size: {len(processor.tokenizer)}\")\n",
    "print(f\"   Min pixels: {min_pix:,} (~256x256 after resize)\")\n",
    "print(f\"   Max pixels: {max_pix:,} (~512x512 after resize)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28579b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Custom data collator for Qwen2-VL multimodal inputs.\n",
    "    \n",
    "    What it does:\n",
    "    1. Applies chat template to convert messages to text format\n",
    "    2. Extracts image paths from message structure\n",
    "    3. Processes both text and images together with the processor\n",
    "    4. Creates labels for causal language modeling\n",
    "    \"\"\"\n",
    "    texts = [\n",
    "        processor.apply_chat_template(\n",
    "            example[\"messages\"], \n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for example in examples\n",
    "    ]\n",
    "\n",
    "    image_inputs = []\n",
    "    for example in examples:\n",
    "        images = []\n",
    "        for msg in example[\"messages\"]:\n",
    "            if isinstance(msg[\"content\"], list):\n",
    "                for item in msg[\"content\"]:\n",
    "                    if isinstance(item, dict) and item.get(\"type\") == \"image\":\n",
    "                        images.append(item[\"image\"])\n",
    "        image_inputs.append(images if images else None)\n",
    "\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "    \n",
    "    return batch\n",
    "\n",
    "print(\"Custom collator function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration using SFTConfig\n",
    "training_args = SFTConfig(\n",
    "    # Basic settings\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    # Batch size (A100-optimized)\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # Effective batch = 16\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=False,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # A100-specific optimizations\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    tf32=True,  # A100 TensorFloat-32\n",
    "    bf16=True,\n",
    "    # Training hyperparameters\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    # Logging\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    "    # Evaluation & checkpointing\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    # VLM-specific (CRITICAL!)\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    max_length=384,\n",
    "    packing=False,\n",
    "    # A100 performance\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_persistent_workers=True,\n",
    "    dataloader_prefetch_factor=8,\n",
    "    # Reproducibility\n",
    "    torch_empty_cache_steps=20,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(\n",
    "    f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\"\n",
    ")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Mixed precision: bf16={training_args.bf16}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18c538-0319-4f4e-9286-7445b412d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    \"\"\"Clean up GPU memory between epochs\"\"\"\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"\\n🧹 GPU memory cleaned up\")\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8aa639-919c-451d-89d1-66e43b908e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class PeriodicCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Saves a rolling checkpoint every N steps (keeps only 1 copy).\n",
    "    This is separate from epoch-based checkpoints.\n",
    "    \n",
    "    Purpose: Resume training if crash happens mid-epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_steps=10, checkpoint_dir=\"periodic_checkpoint\"):\n",
    "        self.save_steps = save_steps\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Save every N steps\n",
    "        if state.global_step % self.save_steps == 0:\n",
    "            checkpoint_path = os.path.join(args.output_dir, self.checkpoint_dir)\n",
    "            \n",
    "            # Remove old periodic checkpoint\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                print(f\"\\n🔄 Replacing periodic checkpoint at step {state.global_step}\")\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "            else:\n",
    "                print(f\"\\n💾 Creating first periodic checkpoint at step {state.global_step}\")\n",
    "            \n",
    "            # Get model from kwargs (passed by callback handler)\n",
    "            model = kwargs.get(\"model\")\n",
    "            if model is None:\n",
    "                print(\"⚠️  Warning: Model not found in callback kwargs, skipping checkpoint\")\n",
    "                return control\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            model.save_pretrained(checkpoint_path)\n",
    "            \n",
    "            # Save trainer state (properly serialize it)\n",
    "            state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
    "            with open(state_path, 'w') as f:\n",
    "                # Convert TrainerState to dict manually\n",
    "                state_dict = {\n",
    "                    'epoch': state.epoch,\n",
    "                    'global_step': state.global_step,\n",
    "                    'max_steps': state.max_steps,\n",
    "                    'num_train_epochs': state.num_train_epochs,\n",
    "                    'log_history': state.log_history,\n",
    "                    'best_metric': state.best_metric,\n",
    "                    'best_model_checkpoint': state.best_model_checkpoint,\n",
    "                    'is_local_process_zero': state.is_local_process_zero,\n",
    "                    'is_world_process_zero': state.is_world_process_zero,\n",
    "                }\n",
    "                json.dump(state_dict, f, indent=2)\n",
    "            \n",
    "            print(f\"✅ Periodic checkpoint saved (step {state.global_step})\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(\"✅ PeriodicCheckpointCallback defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531b1e4-a664-487e-987a-b783d61f1459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "kernel_pid = os.getpid()\n",
    "print(f\"Your notebook's kernel PID is: {kernel_pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "if FULL_TRAINING_MODE:\n",
    "    eval_dataset_for_trainer = None\n",
    "else:\n",
    "    eval_dataset_for_trainer = validation_dataset\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "if eval_dataset_for_trainer:\n",
    "    print(f\"Validation samples: {len(eval_dataset_for_trainer)}\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset_for_trainer,\n",
    "    data_collator=collate_fn,\n",
    "    processing_class=processor.tokenizer,\n",
    ")\n",
    "trainer.add_callback(MemoryCleanupCallback())\n",
    "\n",
    "if not FULL_TRAINING_MODE and eval_dataset_for_trainer is not None:\n",
    "    # SMAPE callback\n",
    "    smape_eval_samples = min(\n",
    "        len(eval_dataset_for_trainer), \n",
    "        max(100, int(len(eval_dataset_for_trainer) * 0.2))\n",
    "    )\n",
    "    smape_eval_samples = min(smape_eval_samples, 1000)\n",
    "    \n",
    "    smape_callback = SMAPEEvaluationCallback(\n",
    "        trainer=trainer,\n",
    "        validation_dataset=eval_dataset_for_trainer,\n",
    "        processor=processor,\n",
    "        device=\"cuda\",\n",
    "        max_samples=smape_eval_samples,\n",
    "    )\n",
    "    trainer.add_callback(smape_callback)\n",
    "    print(\"\\n✅ SMAPE callback added\")\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStoppingCallback(smape_callback, patience=2)\n",
    "    trainer.add_callback(early_stopping)\n",
    "    print(\"✅ Early stopping added (patience=2)\")\n",
    "else:\n",
    "    smape_callback = None\n",
    "    print(\"\\n⚠️  No SMAPE evaluation (FULL_TRAINING_MODE)\")\n",
    "\n",
    "# ADD PERIODIC CHECKPOINT CALLBACK (Always enabled!)\n",
    "periodic_checkpoint = PeriodicCheckpointCallback(\n",
    "    save_steps=30,  # Save every 10 steps\n",
    "    checkpoint_dir=\"periodic_checkpoint\"  # Single rolling checkpoint\n",
    ")\n",
    "trainer.add_callback(periodic_checkpoint)\n",
    "print(\"✅ Periodic checkpoint callback added (every 10 steps)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf36f1-29e2-4d1e-81f5-99487290b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "kernel_pid = os.getpid()\n",
    "print(f\"Your notebook's kernel PID is: {kernel_pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a05bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INFERENCE_ONLY:    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Starting Fine-Tuning...\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "    if RESUME_FROM_CHECKPOINT:\n",
    "        print(f\"🔄 Resuming from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training Complete!\")\n",
    "    if smape_callback:\n",
    "        print(f\"Best SMAPE achieved: {smape_callback.best_smape:.2f}%\")\n",
    "        print(f\"SMAPE history: {smape_callback.smape_history}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total epochs: {trainer.state.epoch}\")\n",
    "    print(f\"Total steps: {trainer.state.global_step}\")\n",
    "    print(f\"Best checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "\n",
    "    if smape_callback:\n",
    "        print(f\"\\nSMAPE Performance:\")\n",
    "        print(f\"  Initial SMAPE: {smape_callback.smape_history[0]:.2f}%\")\n",
    "        print(f\"  Final SMAPE: {smape_callback.smape_history[-1]:.2f}%\")\n",
    "        print(f\"  Best SMAPE: {smape_callback.best_smape:.2f}%\")\n",
    "        print(\n",
    "            f\"  Improvement: {smape_callback.smape_history[0] - smape_callback.best_smape:.2f}%\"\n",
    "        )\n",
    "        print(f\"\\n  Epoch-by-epoch SMAPE:\")\n",
    "        for i, smape in enumerate(smape_callback.smape_history, 1):\n",
    "            print(f\"    Epoch {i}: {smape:.2f}%\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Saving the Best Model...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"\\n*** Model and processor saved to: {output_dir}\")\n",
    "    print(\"\\nSaved files:\")\n",
    "    print(\"  - adapter_config.json (LoRA configuration)\")\n",
    "    print(\"  - adapter_model.safetensors (LoRA weights)\")\n",
    "    print(\"  - tokenizer files\")\n",
    "    print(\"  - processor configuration\")\n",
    "else:\n",
    "    print(\"\\n***  Skipping training (INFERENCE_ONLY=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6508d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "if RUN_INFERENCE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Running Inference on Test Set...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\", data_files=f\"{DATASET_FOLDER}/test.jsonl\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    checkpoint_path = INFERENCE_CHECKPOINT if INFERENCE_CHECKPOINT else output_dir\n",
    "    if os.path.exists(f\"{checkpoint_path}/adapter_model.safetensors\"):\n",
    "        print(f\"\\n*** Loading adapter from: {checkpoint_path}\")\n",
    "        from peft import PeftModel\n",
    "\n",
    "        try:\n",
    "            _ = model.device\n",
    "        except:\n",
    "            print(\"Loading base model...\")\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            )\n",
    "            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                model_id,\n",
    "                quantization_config=bnb_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "        model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "        model.eval()\n",
    "        print(\"*** Fine-tuned adapter loaded!\")\n",
    "    else:\n",
    "        print(\"***  Using model from training session (no saved adapter found)\")\n",
    "        model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "    batch_size = 16\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(test_dataset), batch_size), desc=\"Inference\"):\n",
    "        batch_end = min(batch_start + batch_size, len(test_dataset))\n",
    "        batch_examples = test_dataset[batch_start:batch_end]\n",
    "\n",
    "        if not isinstance(batch_examples[\"sample_id\"], list):\n",
    "            batch_examples = {k: [v] for k, v in batch_examples.items()}\n",
    "\n",
    "        batch_sample_ids = batch_examples[\"sample_id\"]\n",
    "        batch_messages = batch_examples[\"messages\"]\n",
    "\n",
    "        batch_texts = []\n",
    "        batch_images = []\n",
    "\n",
    "        for messages in batch_messages:\n",
    "            text = processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            batch_texts.append(text)\n",
    "            images = []\n",
    "            for msg in messages:\n",
    "                if isinstance(msg[\"content\"], list):\n",
    "                    for item in msg[\"content\"]:\n",
    "                        if isinstance(item, dict) and item.get(\"type\") == \"image\":\n",
    "                            images.append(item[\"image\"])\n",
    "            batch_images.append(images if images else None)\n",
    "\n",
    "        inputs = processor(\n",
    "            text=batch_texts,\n",
    "            images=batch_images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        for i, (in_ids, out_ids) in enumerate(zip(inputs.input_ids, generated_ids)):\n",
    "            generated_ids_trimmed = out_ids[len(in_ids) :]\n",
    "            prediction = processor.decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            ).strip()\n",
    "\n",
    "            try:\n",
    "                predicted_price = float(prediction)\n",
    "            except (ValueError, TypeError):\n",
    "                print(\n",
    "                    f\"Warning: Non-numeric prediction '{prediction}' for sample {batch_sample_ids[i]}, using 0.0\"\n",
    "                )\n",
    "                predicted_price = 0.0\n",
    "\n",
    "            predictions.append(predicted_price)\n",
    "            sample_ids.append(batch_sample_ids[i])\n",
    "\n",
    "        if (batch_end) % 500 == 0 or batch_end == len(test_dataset):\n",
    "            print(f\"  Processed {batch_end}/{len(test_dataset)} samples...\")\n",
    "\n",
    "    submission_df = pd.DataFrame({\"sample_id\": sample_ids, \"price\": predictions})\n",
    "    submission_path = \"test_out.csv\"\n",
    "\n",
    "    print(\"\\nValidating submission format...\")\n",
    "    assert \"sample_id\" in submission_df.columns, \"Missing sample_id column\"\n",
    "    assert \"price\" in submission_df.columns, \"Missing price column\"\n",
    "    assert len(submission_df) == len(test_dataset), (\n",
    "        f\"Mismatch: {len(submission_df)} predictions vs {len(test_dataset)} test samples\"\n",
    "    )\n",
    "    assert submission_df[\"sample_id\"].duplicated().sum() == 0, (\n",
    "        \"Duplicate sample_ids found!\"\n",
    "    )\n",
    "    print(\"*** Submission format validated\")\n",
    "\n",
    "    failed_predictions = sum(1 for p in predictions if p == 0.0)\n",
    "    if failed_predictions > len(predictions) * 0.1:\n",
    "        print(\n",
    "            f\"\\n***  WARNING: {failed_predictions}/{len(predictions)} predictions failed (returned 0.0)\"\n",
    "        )\n",
    "        print(\"This suggests the model may need more training or better prompts\")\n",
    "\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"*** Submission saved to: {submission_path}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Sample predictions:\")\n",
    "    print(submission_df.head(10))\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n*** GPU memory cleaned up\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n***  Skipping inference (RUN_INFERENCE=False)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (t-2)",
   "language": "python",
   "name": "t-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
