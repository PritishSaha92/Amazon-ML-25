{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c2db2-3343-4938-ac5a-c559ac45044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U transformers datasets peft trl bitsandbytes accelerate qwen-vl-utils pillow tensorboard webdataset unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d9d1c2-bf8b-4f9c-b5b9-fa5bb907f368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/t-2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "‚úÖ Successfully patched unsloth_zoo.tokenizer_utils.fix_untrained_tokens\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GH200 480GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "from unsloth import FastVisionModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "import webdataset as wds\n",
    "from transformers import AutoProcessor, TrainerCallback, EarlyStoppingCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import IterableDataset\n",
    "\n",
    "try:\n",
    "    import unsloth_zoo.tokenizer_utils as tokenizer_utils_module\n",
    "    \n",
    "    # Replace the function with a no-op\n",
    "    def dummy_fix_untrained_tokens(*args, **kwargs):\n",
    "        print(\"‚úÖ Skipping fix_untrained_tokens (patched)\")\n",
    "        return None\n",
    "    \n",
    "    tokenizer_utils_module.fix_untrained_tokens = dummy_fix_untrained_tokens\n",
    "    print(\"‚úÖ Successfully patched unsloth_zoo.tokenizer_utils.fix_untrained_tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not patch tokenizer_utils: {e}\")\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" # <-- ADD THIS\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "# Initial cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01741cb-f8b8-4adf-ab61-7e1c40573e1b",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b80e67-1880-4773-96ed-b5dd203a32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "üöÄ Validation DISABLED: Training on the full dataset.\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN CONFIGURATION ---\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "OUTPUT_DIR = \"./qwen2.5-vl-3b-price-predictor-final\"\n",
    "\n",
    "# --- EXECUTION CONTROL ---\n",
    "# Set to True to enable evaluation, SMAPE, and early stopping\n",
    "USE_VALIDATION = False\n",
    "# Set to True to generate a submission file after training\n",
    "RUN_INFERENCE = True\n",
    "# Set to True to skip training and only run inference\n",
    "INFERENCE_ONLY = False\n",
    "# Set to True or a path to resume training from a checkpoint\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "\n",
    "# --- DATASET PATHS (WebDataset URL Patterns) ---\n",
    "# NOTE: Adjust the shard numbers (e.g., 000066) to match the output from 5_convert.py\n",
    "WEBDATASET_TRAIN_URL = \"./webdataset_train/train-shard-{000000..000067}.tar\"\n",
    "WEBDATASET_VALIDATION_URL = \"./webdataset_validation/validation-shard-{000000..000007}.tar\"\n",
    "WEBDATASET_TEST_URL = \"./webdataset_test/test-shard-{000000..000074}.tar\"\n",
    "\n",
    "\n",
    "# --- TRAINING HYPERPARAMETERS ---\n",
    "BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "NUM_WORKERS = 0\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "TRAIN_SAMPLES = 67499    # From preprocessing output\n",
    "STEPS_PER_EPOCH = TRAIN_SAMPLES // (BATCH_SIZE * GRADIENT_ACCUMULATION)\n",
    "MAX_STEPS = STEPS_PER_EPOCH * NUM_EPOCHS\n",
    "\n",
    "# --- EVALUATION SETTINGS ---\n",
    "# These are only active if USE_VALIDATION = True\n",
    "EVAL_STEPS = 250\n",
    "EARLY_STOPPING_PATIENCE = 3 # Stop after 3 evaluations with no improvement\n",
    "\n",
    "\n",
    "# --- PROCESSOR SETTINGS ---\n",
    "MIN_PIXELS = 256 * 28 * 28\n",
    "MAX_PIXELS = 512 * 28 * 28\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "if USE_VALIDATION:\n",
    "    print(f\"üöÄ Validation ENABLED: Evaluating every {EVAL_STEPS} steps.\")\n",
    "else:\n",
    "    print(\"üöÄ Validation DISABLED: Training on the full dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fde0e-9fd0-404a-a121-01e66d72a287",
   "metadata": {},
   "source": [
    "## Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9153ee-242c-46ed-86f5-64f1dd105072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processor loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Processor loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cef11-d4ee-4fcb-986f-dc9b370c5e62",
   "metadata": {},
   "source": [
    "## WebDataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6383178f-b288-4c96-a455-d50fdd842e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WebDataset functions defined!\n"
     ]
    }
   ],
   "source": [
    "def decode_sample(sample):\n",
    "    \"\"\"Decode preprocessed tensors from WebDataset using np.load to handle .npy format correctly\"\"\"\n",
    "    try:\n",
    "        # ‚úÖ CORRECT: Use np.load() with BytesIO to properly parse .npy format\n",
    "        # .npy files have headers - np.frombuffer() was reading EVERYTHING including header!\n",
    "        input_ids_array = np.load(io.BytesIO(sample[\"input_ids.npy\"]))\n",
    "        attention_mask_array = np.load(io.BytesIO(sample[\"attention_mask.npy\"]))\n",
    "        pixel_values_array = np.load(io.BytesIO(sample[\"pixel_values.npy\"]))\n",
    "        \n",
    "        metadata = json.loads(sample[\"metadata.json\"].decode(\"utf-8\"))\n",
    "        \n",
    "        # Now pixel_values_array has the CORRECT shape from the .npy file!\n",
    "        # It should be 2D: [num_patches, hidden_dim]\n",
    "        \n",
    "        if pixel_values_array.ndim != 2:\n",
    "            raise ValueError(f\"pixel_values has {pixel_values_array.ndim} dims, expected 2D [patches, hidden_dim]\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_ids = torch.from_numpy(input_ids_array).long()\n",
    "        attention_mask = torch.from_numpy(attention_mask_array).long()\n",
    "        pixel_values = torch.from_numpy(pixel_values_array).float()\n",
    "        \n",
    "        image_grid_thw = torch.tensor(metadata[\"image_grid_thw\"], dtype=torch.long)\n",
    "        \n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"image_grid_thw\": image_grid_thw,\n",
    "        }\n",
    "        \n",
    "        if \"sample_id\" in metadata:\n",
    "            result[\"sample_id\"] = metadata[\"sample_id\"]\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Let handler skip this corrupted sample\n",
    "        print(f\"‚ö†Ô∏è Skipping corrupted sample: {e}\")\n",
    "        raise  # Re-raise for wds.warn_and_continue handler\n",
    "\n",
    "\n",
    "def create_webdataset(url_pattern, is_train=True):\n",
    "    \"\"\"Create a WebDataset with proper decoding and error handling\"\"\"\n",
    "    dataset = (\n",
    "        wds.WebDataset(url_pattern, handler=wds.warn_and_continue)\n",
    "        .map(decode_sample, handler=wds.warn_and_continue)  # Handler skips errors\n",
    "    )\n",
    "    \n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "print(\"‚úÖ WebDataset functions defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8397661f-a640-4a31-a266-bddb8c0dbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexableWebDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Wrapper to add __len__ and __getitem__ for Unsloth compatibility\"\"\"\n",
    "    def __init__(self, webdataset, length):\n",
    "        self.dataset = webdataset\n",
    "        self.length = length\n",
    "        self._cache = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Cache first 250 samples for Unsloth's fix_untrained_tokens check\"\"\"\n",
    "        if self._cache is None:\n",
    "            # Build cache on first access\n",
    "            self._cache = {}\n",
    "            iterator = iter(self.dataset)\n",
    "            for i in range(min(250, self.length)):\n",
    "                try:\n",
    "                    self._cache[i] = next(iterator)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        \n",
    "        if idx in self._cache:\n",
    "            return self._cache[idx]\n",
    "        \n",
    "        raise IndexError(f\"Index {idx} out of cacheable range (0-249)\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Forward actual training iteration to WebDataset\"\"\"\n",
    "        return iter(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f319ee53-338f-40de-a29c-808fda267582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WebDatasets...\n",
      "‚úÖ Training dataset loaded from: ./webdataset_train/train-shard-{000000..000067}.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/t-2/lib/python3.12/site-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading WebDatasets...\")\n",
    "\n",
    "wds_train = create_webdataset(WEBDATASET_TRAIN_URL, is_train=True)\n",
    "\n",
    "# Wrap for Unsloth\n",
    "train_dataset = IndexableWebDataset(wds_train, length=TRAIN_SAMPLES)\n",
    "\n",
    "eval_dataset = create_webdataset(\n",
    "    WEBDATASET_VALIDATION_URL, \n",
    "    is_train=False\n",
    ") if USE_VALIDATION else None\n",
    "\n",
    "print(f\"‚úÖ Training dataset loaded from: {WEBDATASET_TRAIN_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cfedaf-380a-4497-a9a6-62150b993570",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabe25b1-a7f7-4153-8478-fe6836e4bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Qwen2.5-VL-3B with Unsloth optimization...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.1: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 94.5 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Qwen2_5_Vl does not support SDPA - switching to fast eager.\n",
      "\n",
      "üîß Fixing embedding size mismatch...\n",
      "   Tokenizer vocab size: 151665\n",
      "   Model vocab size BEFORE: 151936\n",
      "   Model vocab size AFTER: 151936\n",
      "‚úÖ Embedding layer resized! Vision tokens can now be processed.\n",
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "‚úÖ Training config: 3 epochs, batch=4, workers=0\n",
      "‚úÖ Model loaded and SFTConfig configured!\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Loading Qwen2.5-VL-3B with Unsloth optimization...\")\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=MODEL_ID,\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    use_gradient_checkpointing=False, # We found this was a bottleneck\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nüîß Fixing embedding size mismatch...\")\n",
    "print(f\"   Tokenizer vocab size: {len(processor.tokenizer)}\")\n",
    "print(f\"   Model vocab size BEFORE: {model.config.vocab_size}\")\n",
    "\n",
    "# Resize the embedding layer to handle ALL tokens (including vision tokens)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "print(f\"   Model vocab size AFTER: {model.config.vocab_size}\")\n",
    "print(\"‚úÖ Embedding layer resized! Vision tokens can now be processed.\")\n",
    "\n",
    "\n",
    "# model = FastVisionModel.get_peft_model(\n",
    "#     model,\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "#                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     bias=\"none\",\n",
    "#     use_gradient_checkpointing=False,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,      # Keep vision layers trainable\n",
    "    finetune_language_layers=True,     # Keep language layers trainable\n",
    "    finetune_attention_modules=True,   # Keep attention trainable\n",
    "    finetune_mlp_modules=True,         # Keep MLP trainable\n",
    "    r=16,                              # LoRA rank\n",
    "    lora_alpha=16,                     # LoRA alpha\n",
    "    lora_dropout=0.,                 # Dropout\n",
    "    bias=\"none\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# --- SFTConfig with Conditional Evaluation ---\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=MAX_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=EVAL_STEPS,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataset_text_field=\"\",\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    dataloader_pin_memory=False,\n",
    "    torch_empty_cache_steps=20,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Add validation settings if using validation\n",
    "if USE_VALIDATION:\n",
    "    training_args.eval_strategy = \"steps\"  # ‚Üê CHANGE from \"epoch\"\n",
    "    training_args.eval_steps = EVAL_STEPS\n",
    "    training_args.metric_for_best_model = \"eval_loss\"  # Use loss, not SMAPE\n",
    "    training_args.greater_is_better = False\n",
    "    training_args.load_best_model_at_end = True\n",
    "\n",
    "print(f\"‚úÖ Training config: {NUM_EPOCHS} epochs, batch={BATCH_SIZE}, workers={NUM_WORKERS}\")\n",
    "print(\"‚úÖ Model loaded and SFTConfig configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23632505-a442-46e2-8210-d3216e724551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate with proper special token masking\"\"\"\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [example[\"input_ids\"] for example in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [example[\"attention_mask\"] for example in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    \n",
    "    pixel_values = torch.cat([example[\"pixel_values\"] for example in batch], dim=0)\n",
    "    image_grid_thw = torch.stack([example[\"image_grid_thw\"] for example in batch])\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # ‚úÖ THE KEY FIX: Mask ALL special tokens >= vocab_size\n",
    "    vocab_size = processor.tokenizer.vocab_size\n",
    "    labels[labels >= vocab_size] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"image_grid_thw\": image_grid_thw,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146af7c8-9eae-49e8-8401-edea2a979228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Periodic checkpoint callback defined!\n"
     ]
    }
   ],
   "source": [
    "class PeriodicCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"Save checkpoint every N steps (overwrites previous)\"\"\"\n",
    "    def __init__(self, save_steps=60):\n",
    "        self.save_steps = save_steps\n",
    "        self.last_save_step = 0\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step - self.last_save_step >= self.save_steps:\n",
    "            checkpoint_dir = os.path.join(args.output_dir, \"checkpoint-periodic\")\n",
    "            kwargs[\"model\"].save_pretrained(checkpoint_dir)\n",
    "            print(f\"\\nüíæ Periodic checkpoint saved at step {state.global_step}\")\n",
    "            self.last_save_step = state.global_step\n",
    "        return control\n",
    "\n",
    "print(\"‚úÖ Periodic checkpoint callback defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e278175-3557-445b-a1fa-20ff59243cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING TRAINER\n",
      "============================================================\n",
      "Loading trainer...\n",
      "‚úÖ Skipping fix_untrained_tokens (patched)\n",
      "‚úÖ Trainer initialized!\n"
     ]
    }
   ],
   "source": [
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    \"\"\"Clean up GPU memory between epochs\"\"\"\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        print(\"\\nüßπ GPU memory cleaned\")\n",
    "        return control\n",
    "\n",
    "if not INFERENCE_ONLY:\n",
    "    print(\"=\"*60)\n",
    "    print(\"INITIALIZING TRAINER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load validation dataset if enabled\n",
    "    eval_dataset_for_trainer = None\n",
    "    if USE_VALIDATION:\n",
    "        print(f\"Loading validation WebDataset from: {WEBDATASET_VALIDATION_URL}\")\n",
    "        eval_dataset_for_trainer = eval_dataset  # ‚Üê Use the one you already created!\n",
    "    \n",
    "    print(\"Loading trainer...\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # ‚Üê Plain IterableDataset (no wrapper!)\n",
    "        eval_dataset=eval_dataset_for_trainer,\n",
    "        data_collator=collate_fn,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.add_callback(MemoryCleanupCallback())\n",
    "    trainer.add_callback(PeriodicCheckpointCallback(save_steps=60))\n",
    "    \n",
    "    if USE_VALIDATION:\n",
    "        trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48db702-566c-4e64-8bc6-fc3fb9d61ff7",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c1c55fe-e7e8-47be-ab28-0f385fad9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEBUGGING BATCH DATA\n",
      "============================================================\n",
      "\n",
      "üì¶ Fetching one raw sample...\n",
      "\n",
      "‚úÖ Raw sample keys: dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw', 'sample_id', '__key__'])\n",
      "   input_ids shape: torch.Size([716])\n",
      "   pixel_values shape: torch.Size([1296, 1176])\n",
      "   image_grid_thw shape: torch.Size([3])\n",
      "\n",
      "üîç Raw input_ids statistics:\n",
      "   Min: 4\n",
      "   Max: 151655\n",
      "   Unique tokens: 209\n",
      "   Vision tokens found: [151652, 151653, 151655]\n",
      "\n",
      "üîß Testing collate_fn with batch_size=1...\n",
      "   ‚úÖ Collation successful!\n",
      "\n",
      "üìä Collated batch shapes:\n",
      "   input_ids: torch.Size([1, 716])\n",
      "   attention_mask: torch.Size([1, 716])\n",
      "   pixel_values: torch.Size([1296, 1176])\n",
      "   image_grid_thw: torch.Size([1, 3])\n",
      "   labels: torch.Size([1, 716])\n",
      "\n",
      "üîç Labels statistics:\n",
      "   Min (non-masked): 4\n",
      "   Max (non-masked): 96693\n",
      "   Masked tokens (-100): 332\n",
      "   Total tokens: 716\n",
      "   ‚úÖ All vision tokens properly masked\n",
      "\n",
      "üìö Model vocabulary:\n",
      "   Tokenizer vocab_size: 151643\n",
      "   Model config vocab_size: 151936\n",
      "   ‚úÖ All labels within vocab range\n",
      "\n",
      "\n",
      "üß™ Testing with batch_size=4...\n",
      "   Collected 4 samples\n",
      "   ‚úÖ Batch collation successful!\n",
      "   Batch shapes: input_ids=torch.Size([4, 1007]), labels=torch.Size([4, 1007])\n",
      "   Labels range: 1 to 96693\n",
      "   ‚úÖ All labels valid\n",
      "\n",
      "============================================================\n",
      "DEBUG COMPLETE - Share all output above!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DEBUG: Test a single batch to find the exact problem\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEBUGGING BATCH DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Get a single raw sample from the dataset\n",
    "print(\"\\nüì¶ Fetching one raw sample...\")\n",
    "raw_sample = next(iter(train_dataset))\n",
    "\n",
    "print(\"\\n‚úÖ Raw sample keys:\", raw_sample.keys())\n",
    "print(f\"   input_ids shape: {raw_sample['input_ids'].shape}\")\n",
    "print(f\"   pixel_values shape: {raw_sample['pixel_values'].shape}\")\n",
    "print(f\"   image_grid_thw shape: {raw_sample['image_grid_thw'].shape}\")\n",
    "\n",
    "# 2. Check raw input_ids values\n",
    "print(\"\\nüîç Raw input_ids statistics:\")\n",
    "print(f\"   Min: {raw_sample['input_ids'].min().item()}\")\n",
    "print(f\"   Max: {raw_sample['input_ids'].max().item()}\")\n",
    "print(f\"   Unique tokens: {torch.unique(raw_sample['input_ids']).numel()}\")\n",
    "\n",
    "# Find vision tokens\n",
    "vision_tokens = raw_sample['input_ids'][(raw_sample['input_ids'] >= 151650) & (raw_sample['input_ids'] <= 151660)]\n",
    "print(f\"   Vision tokens found: {torch.unique(vision_tokens).tolist()}\")\n",
    "\n",
    "# 3. Test collate_fn on a single-item batch\n",
    "print(\"\\nüîß Testing collate_fn with batch_size=1...\")\n",
    "batch = [raw_sample]\n",
    "try:\n",
    "    collated = collate_fn(batch)\n",
    "    print(\"   ‚úÖ Collation successful!\")\n",
    "    \n",
    "    print(\"\\nüìä Collated batch shapes:\")\n",
    "    print(f\"   input_ids: {collated['input_ids'].shape}\")\n",
    "    print(f\"   attention_mask: {collated['attention_mask'].shape}\")\n",
    "    print(f\"   pixel_values: {collated['pixel_values'].shape}\")\n",
    "    print(f\"   image_grid_thw: {collated['image_grid_thw'].shape}\")\n",
    "    print(f\"   labels: {collated['labels'].shape}\")\n",
    "    \n",
    "    print(\"\\nüîç Labels statistics:\")\n",
    "    print(f\"   Min (non-masked): {collated['labels'][collated['labels'] != -100].min().item()}\")\n",
    "    print(f\"   Max (non-masked): {collated['labels'][collated['labels'] != -100].max().item()}\")\n",
    "    print(f\"   Masked tokens (-100): {(collated['labels'] == -100).sum().item()}\")\n",
    "    print(f\"   Total tokens: {collated['labels'].numel()}\")\n",
    "    \n",
    "    # Check for vision tokens in labels (SHOULD BE -100!)\n",
    "    labels_flat = collated['labels'].view(-1)\n",
    "    vision_in_labels = labels_flat[(labels_flat >= 151650) & (labels_flat <= 151660) & (labels_flat != -100)]\n",
    "    if len(vision_in_labels) > 0:\n",
    "        print(f\"   ‚ö†Ô∏è WARNING: Found {len(vision_in_labels)} vision tokens NOT masked!\")\n",
    "        print(f\"   Vision tokens in labels: {torch.unique(vision_in_labels).tolist()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All vision tokens properly masked\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Collation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. Check vocab size\n",
    "print(\"\\nüìö Model vocabulary:\")\n",
    "print(f\"   Tokenizer vocab_size: {processor.tokenizer.vocab_size}\")\n",
    "print(f\"   Model config vocab_size: {model.config.vocab_size}\")\n",
    "\n",
    "# Check if any labels exceed vocab size\n",
    "if 'collated' in locals():\n",
    "    labels_no_mask = collated['labels'][collated['labels'] != -100]\n",
    "    if len(labels_no_mask) > 0:\n",
    "        max_label = labels_no_mask.max().item()\n",
    "        if max_label >= processor.tokenizer.vocab_size:\n",
    "            print(f\"   ‚ùå PROBLEM: Max label {max_label} >= vocab_size {processor.tokenizer.vocab_size}\")\n",
    "            print(f\"   Out-of-bounds labels: {labels_no_mask[labels_no_mask >= processor.tokenizer.vocab_size].tolist()}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All labels within vocab range\")\n",
    "\n",
    "# 5. Test with multiple samples (batch_size=4)\n",
    "print(\"\\n\\nüß™ Testing with batch_size=4...\")\n",
    "batch_4 = []\n",
    "dataset_iter = iter(train_dataset)\n",
    "for i in range(4):\n",
    "    try:\n",
    "        batch_4.append(next(dataset_iter))\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "print(f\"   Collected {len(batch_4)} samples\")\n",
    "\n",
    "try:\n",
    "    collated_4 = collate_fn(batch_4)\n",
    "    print(\"   ‚úÖ Batch collation successful!\")\n",
    "    print(f\"   Batch shapes: input_ids={collated_4['input_ids'].shape}, labels={collated_4['labels'].shape}\")\n",
    "    \n",
    "    # Check labels again\n",
    "    labels_no_mask = collated_4['labels'][collated_4['labels'] != -100]\n",
    "    if len(labels_no_mask) > 0:\n",
    "        print(f\"   Labels range: {labels_no_mask.min().item()} to {labels_no_mask.max().item()}\")\n",
    "        if labels_no_mask.max().item() >= processor.tokenizer.vocab_size:\n",
    "            print(f\"   ‚ùå Found out-of-bounds labels!\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All labels valid\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Batch collation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUG COMPLETE - Share all output above!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717628cf-5ea4-4dd8-8506-35f239d24da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INSPECTING RAW INPUT_IDS FROM WEBDATASET\n",
      "\n",
      "Token IDs shape: torch.Size([489])\n",
      "Min ID: 11\n",
      "Max ID: 151655\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç INSPECTING RAW INPUT_IDS FROM WEBDATASET\")\n",
    "raw_sample = next(iter(train_dataset))\n",
    "\n",
    "# Get the actual token IDs\n",
    "token_ids = raw_sample['input_ids']\n",
    "\n",
    "print(f\"\\nToken IDs shape: {token_ids.shape}\")\n",
    "print(f\"Min ID: {token_ids.min().item()}\")\n",
    "print(f\"Max ID: {token_ids.max().item()}\")\n",
    "\n",
    "# Find the HUGE values\n",
    "huge_values = token_ids[token_ids > 151660]\n",
    "if len(huge_values) > 0:\n",
    "    print(f\"\\n‚ùå PROBLEM FOUND!\")\n",
    "    print(f\"   Found {len(huge_values)} tokens with IDs > 151660\")\n",
    "    print(f\"   These values: {huge_values.tolist()[:20]}\")  # Show first 20\n",
    "    print(f\"\\n   Positions: {torch.where(token_ids > 151660)[0].tolist()[:10]}\")\n",
    "    \n",
    "    # Show what's around them\n",
    "    for pos in torch.where(token_ids > 151660)[0][:3]:\n",
    "        start = max(0, pos-5)\n",
    "        end = min(len(token_ids), pos+6)\n",
    "        print(f\"\\n   Context around position {pos}:\")\n",
    "        print(f\"   {token_ids[start:end].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712eb083-5975-4dcb-9a8e-c30b91e2d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç CHECKING image_grid_thw VALUES\n",
      "\n",
      "Sample 0:\n",
      "   image_grid_thw: tensor([ 1, 44, 26])\n",
      "   pixel_values shape: torch.Size([1144, 1176])\n",
      "   Expected patches (t*h*w): 1144\n",
      "   Actual patches: 1144\n",
      "   Match: True\n",
      "\n",
      "Sample 1:\n",
      "   image_grid_thw: tensor([ 1, 36, 36])\n",
      "   pixel_values shape: torch.Size([1296, 1176])\n",
      "   Expected patches (t*h*w): 1296\n",
      "   Actual patches: 1296\n",
      "   Match: True\n",
      "\n",
      "Sample 2:\n",
      "   image_grid_thw: tensor([ 1, 36, 36])\n",
      "   pixel_values shape: torch.Size([1296, 1176])\n",
      "   Expected patches (t*h*w): 1296\n",
      "   Actual patches: 1296\n",
      "   Match: True\n",
      "\n",
      "Sample 3:\n",
      "   image_grid_thw: tensor([ 1, 36, 36])\n",
      "   pixel_values shape: torch.Size([1296, 1176])\n",
      "   Expected patches (t*h*w): 1296\n",
      "   Actual patches: 1296\n",
      "   Match: True\n",
      "\n",
      "Sample 4:\n",
      "   image_grid_thw: tensor([ 1, 36, 36])\n",
      "   pixel_values shape: torch.Size([1296, 1176])\n",
      "   Expected patches (t*h*w): 1296\n",
      "   Actual patches: 1296\n",
      "   Match: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç CHECKING image_grid_thw VALUES\")\n",
    "for i, sample in enumerate(iter(train_dataset)):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"   image_grid_thw: {sample['image_grid_thw']}\")\n",
    "    print(f\"   pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "    \n",
    "    t, h, w = sample['image_grid_thw']\n",
    "    expected = t * h * w\n",
    "    actual = sample['pixel_values'].shape[0]\n",
    "    \n",
    "    print(f\"   Expected patches (t*h*w): {expected}\")\n",
    "    print(f\"   Actual patches: {actual}\")\n",
    "    print(f\"   Match: {expected == actual}\")\n",
    "    \n",
    "    if expected != actual:\n",
    "        print(f\"   ‚ùå MISMATCH FOUND!\")\n",
    "        break\n",
    "    \n",
    "    if i >= 4:  # Check first 5 samples\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b31b83d-c846-442b-a406-7489314c316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [71,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [116,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [117,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [106,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [0,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [1,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [2,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [3,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [4,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [5,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [6,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [7,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [8,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [9,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [10,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [11,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [12,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [13,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [16,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [17,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [18,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [19,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [20,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [21,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [22,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [23,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [24,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [25,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [26,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [27,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [28,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [29,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [98,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [226,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [211,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSTARTING TRAINING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESUME_FROM_CHECKPOINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# When load_best_model_at_end=True, the trainer already holds the best model.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# We just need to save it.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lambda/nfs/amazon-other/student_resource/src/unsloth_compiled_cache/UnslothSFTTrainer.py:53\u001b[39m, in \u001b[36mprepare_for_training_mode.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_training\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.for_training()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m output = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfor_inference\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lambda/nfs/amazon-other/student_resource/src/unsloth_compiled_cache/UnslothSFTTrainer.py:1040\u001b[39m, in \u001b[36m_UnslothSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:40\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lambda/nfs/amazon-other/student_resource/src/unsloth_compiled_cache/UnslothSFTTrainer.py:1029\u001b[39m, in \u001b[36m_UnslothSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs = \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/unsloth/models/_utils.py:1321\u001b[39m, in \u001b[36m_unsloth_pre_compute_loss\u001b[39m\u001b[34m(self, model, inputs, *args, **kwargs)\u001b[39m\n\u001b[32m   1315\u001b[39m     logger.warning_once(\n\u001b[32m   1316\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m     )\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/trainer.py:4099\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4097\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4098\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4099\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4100\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4101\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/peft/peft_model.py:1850\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1849\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1861\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1863\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1485\u001b[39m, in \u001b[36mQwen2_5_VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1480\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m   1481\u001b[39m output_hidden_states = (\n\u001b[32m   1482\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1483\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m=\u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1504\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1506\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1265\u001b[39m, in \u001b[36mQwen2_5_VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[39m\n\u001b[32m   1262\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.get_input_embeddings()(input_ids)\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m     image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m     image_embeds = torch.cat(image_embeds, dim=\u001b[32m0\u001b[39m).to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m   1267\u001b[39m     image_mask, _ = \u001b[38;5;28mself\u001b[39m.get_placeholder_mask(\n\u001b[32m   1268\u001b[39m         input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n\u001b[32m   1269\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1178\u001b[39m, in \u001b[36mQwen2_5_VLModel.get_image_features\u001b[39m\u001b[34m(self, pixel_values, image_grid_thw)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[33;03mEncodes images into continuous embeddings that can be forwarded to the language model.\u001b[39;00m\n\u001b[32m   1170\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1175\u001b[39m \u001b[33;03m        The temporal, height and width of feature shape of each image in LLM.\u001b[39;00m\n\u001b[32m   1176\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1177\u001b[39m pixel_values = pixel_values.type(\u001b[38;5;28mself\u001b[39m.visual.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m split_sizes = (image_grid_thw.prod(-\u001b[32m1\u001b[39m) // \u001b[38;5;28mself\u001b[39m.visual.spatial_merge_size**\u001b[32m2\u001b[39m).tolist()\n\u001b[32m   1180\u001b[39m image_embeds = torch.split(image_embeds, split_sizes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/t-2/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:443\u001b[39m, in \u001b[36mQwen2_5_VisionTransformerPretrainedModel.forward\u001b[39m\u001b[34m(self, hidden_states, grid_thw, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m hidden_states = hidden_states.reshape(seq_len, -\u001b[32m1\u001b[39m)\n\u001b[32m    442\u001b[39m rotary_pos_emb = rotary_pos_emb.reshape(seq_len // \u001b[38;5;28mself\u001b[39m.spatial_merge_unit, \u001b[38;5;28mself\u001b[39m.spatial_merge_unit, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m rotary_pos_emb = \u001b[43mrotary_pos_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwindow_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    444\u001b[39m rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -\u001b[32m1\u001b[39m)\n\u001b[32m    445\u001b[39m emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if not INFERENCE_ONLY:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    \n",
    "    # When load_best_model_at_end=True, the trainer already holds the best model.\n",
    "    # We just need to save it.\n",
    "    if USE_VALIDATION:\n",
    "        print(f\"Saving the best model from checkpoint: {trainer.state.best_model_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Saving final model...\")\n",
    "        \n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"‚úÖ Model and processor saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b1194-ca1a-4c0c-af71-3560ce27f1f6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd64873-4843-44e5-8952-87bf3820e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_INFERENCE:\n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING INFERENCE ON TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get the model\n",
    "    inference_model = trainer.model if hasattr(trainer, 'model') else model\n",
    "    inference_model.eval()\n",
    "    \n",
    "    # Load test WebDataset\n",
    "    test_wds = create_webdataset(WEBDATASET_TEST_URL, is_train=False)\n",
    "    \n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "    batch = []\n",
    "    \n",
    "    print(\"Processing test samples...\")\n",
    "    \n",
    "    for sample in tqdm(test_wds, desc=\"Inference\", total=75000):\n",
    "        batch.append(sample)\n",
    "        \n",
    "        # Process batch when full\n",
    "        if len(batch) == EVAL_BATCH_SIZE:\n",
    "            # Use collate_fn to prepare batch (already preprocessed!)\n",
    "            batch_data = collate_fn(batch)\n",
    "            \n",
    "            # Move to GPU (remove labels for inference)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch_data[\"input_ids\"].to(\"cuda\"),\n",
    "                \"attention_mask\": batch_data[\"attention_mask\"].to(\"cuda\"),\n",
    "                \"pixel_values\": batch_data[\"pixel_values\"].to(\"cuda\"),\n",
    "                \"image_grid_thw\": batch_data[\"image_grid_thw\"].to(\"cuda\"),\n",
    "            }\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                generated_ids = inference_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    num_beams=1,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                )\n",
    "            \n",
    "            # Decode only the generated part\n",
    "            for i, (in_ids, out_ids) in enumerate(zip(inputs[\"input_ids\"], generated_ids)):\n",
    "                generated_ids_trimmed = out_ids[len(in_ids):]\n",
    "                prediction = processor.decode(\n",
    "                    generated_ids_trimmed,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False,\n",
    "                ).strip()\n",
    "                \n",
    "                # Extract price\n",
    "                try:\n",
    "                    predicted_price = float(prediction)\n",
    "                except:\n",
    "                    predicted_price = 0.0\n",
    "                \n",
    "                predictions.append(predicted_price)\n",
    "                sample_ids.append(batch[i][\"sample_id\"])\n",
    "            \n",
    "            # Clear batch\n",
    "            batch = []\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del inputs, generated_ids\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Handle remaining samples\n",
    "    if len(batch) > 0:\n",
    "        batch_data = collate_fn(batch)\n",
    "        inputs = {\n",
    "            \"input_ids\": batch_data[\"input_ids\"].to(\"cuda\"),\n",
    "            \"attention_mask\": batch_data[\"attention_mask\"].to(\"cuda\"),\n",
    "            \"pixel_values\": batch_data[\"pixel_values\"].to(\"cuda\"),\n",
    "            \"image_grid_thw\": batch_data[\"image_grid_thw\"].to(\"cuda\"),\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = inference_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        for i, (in_ids, out_ids) in enumerate(zip(inputs[\"input_ids\"], generated_ids)):\n",
    "            generated_ids_trimmed = out_ids[len(in_ids):]\n",
    "            prediction = processor.decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            ).strip()\n",
    "            \n",
    "            try:\n",
    "                predicted_price = float(prediction)\n",
    "            except:\n",
    "                predicted_price = 0.0\n",
    "            \n",
    "            predictions.append(predicted_price)\n",
    "            sample_ids.append(batch[i][\"sample_id\"])\n",
    "    \n",
    "    # Create submission\n",
    "    submission_df = pd.DataFrame({\"sample_id\": sample_ids, \"price\": predictions})\n",
    "    submission_df.to_csv(\"test_out.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inference complete!\")\n",
    "    print(f\"   Total predictions: {len(predictions):,}\")\n",
    "    print(f\"   Saved to: test_out.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e9336-fad8-4416-94c6-9cf499ffea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# POST-PROCESSING: FILL MISSING PREDICTIONS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECKING FOR MISSING PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load test CSV and submission\n",
    "test_csv = pd.read_csv(\"../dataset/test.csv\")\n",
    "submission_df = pd.read_csv(\"test_out.csv\")\n",
    "\n",
    "# Find missing sample IDs\n",
    "all_test_ids = set(test_csv[\"sample_id\"].tolist())\n",
    "predicted_ids = set(submission_df[\"sample_id\"].tolist())\n",
    "missing_ids = sorted(all_test_ids - predicted_ids)\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"‚ö†Ô∏è  Found {len(missing_ids)} missing predictions\")\n",
    "    print(f\"   Sample IDs: {missing_ids}\")\n",
    "    \n",
    "    # Use training data mean as fallback\n",
    "    train_csv = pd.read_csv(\"../dataset/train.csv\")\n",
    "    mean_price = train_csv[\"price\"].mean()\n",
    "    median_price = train_csv[\"price\"].median()\n",
    "    \n",
    "    print(f\"\\nüí° Fallback strategy:\")\n",
    "    print(f\"   Training mean price: ${mean_price:.2f}\")\n",
    "    print(f\"   Training median price: ${median_price:.2f}\")\n",
    "    print(f\"   Using MEDIAN (more robust to outliers)\")\n",
    "    \n",
    "    # Create missing predictions with median\n",
    "    missing_rows = pd.DataFrame({\n",
    "        \"sample_id\": missing_ids,\n",
    "        \"price\": [median_price] * len(missing_ids)\n",
    "    })\n",
    "    \n",
    "    # Add to submission\n",
    "    submission_df = pd.concat([submission_df, missing_rows], ignore_index=True)\n",
    "    submission_df = submission_df.sort_values(\"sample_id\").reset_index(drop=True)\n",
    "    submission_df.to_csv(\"test_out.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Added {len(missing_ids)} predictions with median price: ${median_price:.2f}\")\n",
    "    print(f\"   Impact: {len(missing_ids)/75000*100:.4f}% of test set\")\n",
    "else:\n",
    "    print(\"‚úÖ All 75,000 predictions present!\")\n",
    "\n",
    "# Verify final count\n",
    "print(f\"\\nüìä Final submission stats:\")\n",
    "print(f\"   Total rows: {len(submission_df)}\")\n",
    "print(f\"   Sample ID range: {submission_df['sample_id'].min()} - {submission_df['sample_id'].max()}\")\n",
    "print(f\"   Price range: ${submission_df['price'].min():.2f} - ${submission_df['price'].max():.2f}\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (t-2)",
   "language": "python",
   "name": "t-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
